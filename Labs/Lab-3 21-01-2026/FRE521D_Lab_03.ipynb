{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FRE 521D: Data Analytics in Climate, Food and Environment\n",
    "## Lab 3: Python Wrangling - Tidy Data, Types, and Validation\n",
    "\n",
    "**Program:** UBC Master of Food and Resource Economics  \n",
    "**Instructor:** Asif Ahmed Neloy\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"background-color: #FFF3CD; border-left: 4px solid #E6A23C; padding: 15px; margin: 15px 0;\">\n",
    "    <h3 style=\"margin-top: 0; color: #856404;\">Submission Deadline</h3>\n",
    "    <p style=\"margin-bottom: 0; font-size: 1.2em;\"><strong>Wednesday, January 21, 2026 - 11:59 PM (End of Day)</strong></p>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## Lab Objectives\n",
    "\n",
    "In this lab, you will apply Python wrangling techniques to the climate-agriculture data from Assignment 1. You will:\n",
    "\n",
    "1. **Read** data from your MySQL database tables\n",
    "2. **Check and convert** data types appropriately\n",
    "3. **Reshape** data from wide to long format using `pd.melt()`\n",
    "4. **Analyze and handle** missing data\n",
    "5. **Validate** data quality with range, null, and uniqueness checks\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Assignment 1 completed (tables created in your MySQL database)\n",
    "- Docker container running with MySQL\n",
    "- Conda environment activated\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries and Connect to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Pandas version: 2.3.3\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to MySQL successfully!\n"
     ]
    }
   ],
   "source": [
    "# Database connection configuration\n",
    "# Update these values if your setup is different\n",
    "\n",
    "DB_CONFIG = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 3306,\n",
    "    \"user\": \"mfre521d_user\",\n",
    "    \"password\": \"mfre521d_user_pw\",\n",
    "    \"database\": \"mfre521d\",\n",
    "}\n",
    "\n",
    "def get_connection():\n",
    "    \"\"\"Create and return a database connection.\"\"\"\n",
    "    return mysql.connector.connect(**DB_CONFIG)\n",
    "\n",
    "def read_table(query):\n",
    "    \"\"\"Execute a SQL query and return results as DataFrame.\"\"\"\n",
    "    conn = get_connection()\n",
    "    try:\n",
    "        df = pd.read_sql(query, conn)\n",
    "        return df\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    conn = get_connection()\n",
    "    print(\"Connected to MySQL successfully!\")\n",
    "    conn.close()\n",
    "except Error as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Make sure your Docker container is running.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Question 1: Read Data and Check Types\n",
    "\n",
    "## Task: Load Data from A1 Tables and Inspect Data Types\n",
    "\n",
    "As discussed in Lecture 6, **always check `df.dtypes` after loading data**. Different sources may encode the same information differently.\n",
    "\n",
    "### Part A: Read the tables from your database\n",
    "\n",
    "The solution for reading from the database is provided below.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries: 34 rows\n",
      "Crop Production: 4187 rows\n",
      "Temperature Anomalies: 1137 rows\n",
      "\n",
      "Data loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION PROVIDED: Read data from A1 tables\n",
    "\n",
    "# Read countries table\n",
    "df_countries = read_table(\"SELECT * FROM countries\")\n",
    "print(f\"Countries: {len(df_countries)} rows\")\n",
    "\n",
    "# Read crop_production table\n",
    "df_crops = read_table(\"SELECT * FROM crop_production\")\n",
    "print(f\"Crop Production: {len(df_crops)} rows\")\n",
    "\n",
    "# Read temperature_anomalies table\n",
    "df_temp = read_table(\"SELECT * FROM temperature_anomalies\")\n",
    "print(f\"Temperature Anomalies: {len(df_temp)} rows\")\n",
    "\n",
    "print(\"\\nData loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COUNTRIES TABLE\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_id</th>\n",
       "      <th>country_name</th>\n",
       "      <th>iso3_code</th>\n",
       "      <th>region</th>\n",
       "      <th>income_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>ARG</td>\n",
       "      <td>South America</td>\n",
       "      <td>Upper middle income</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Australia</td>\n",
       "      <td>AUS</td>\n",
       "      <td>Oceania</td>\n",
       "      <td>High income</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>BGD</td>\n",
       "      <td>South Asia</td>\n",
       "      <td>Lower middle income</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>BRA</td>\n",
       "      <td>South America</td>\n",
       "      <td>Upper middle income</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Canada</td>\n",
       "      <td>CAN</td>\n",
       "      <td>North America</td>\n",
       "      <td>High income</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   country_id country_name iso3_code         region         income_group\n",
       "0           1    Argentina       ARG  South America  Upper middle income\n",
       "1           2    Australia       AUS        Oceania          High income\n",
       "2           3   Bangladesh       BGD     South Asia  Lower middle income\n",
       "3           4       Brazil       BRA  South America  Upper middle income\n",
       "4           5       Canada       CAN  North America          High income"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View sample of each table\n",
    "print(\"=\" * 60)\n",
    "print(\"COUNTRIES TABLE\")\n",
    "print(\"=\" * 60)\n",
    "df_countries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CROP PRODUCTION TABLE\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>production_id</th>\n",
       "      <th>country_id</th>\n",
       "      <th>year</th>\n",
       "      <th>crop</th>\n",
       "      <th>area_harvested_ha</th>\n",
       "      <th>production_tonnes</th>\n",
       "      <th>yield_kg_ha</th>\n",
       "      <th>fertilizer_use_kg_ha</th>\n",
       "      <th>irrigation_pct</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2001</td>\n",
       "      <td>Soybeans</td>\n",
       "      <td>3751494.0</td>\n",
       "      <td>12036421.75</td>\n",
       "      <td>3208.43</td>\n",
       "      <td>100.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>1993</td>\n",
       "      <td>Maize</td>\n",
       "      <td>2112762.0</td>\n",
       "      <td>11377270.55</td>\n",
       "      <td>5385.02</td>\n",
       "      <td>19.14</td>\n",
       "      <td>9.8</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>1995</td>\n",
       "      <td>Soybeans</td>\n",
       "      <td>1650777.0</td>\n",
       "      <td>7474101.16</td>\n",
       "      <td>4527.63</td>\n",
       "      <td>193.84</td>\n",
       "      <td>56.6</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>2018</td>\n",
       "      <td>Wheat</td>\n",
       "      <td>4782989.0</td>\n",
       "      <td>32397951.41</td>\n",
       "      <td>6773.58</td>\n",
       "      <td>205.12</td>\n",
       "      <td>62.5</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>2013</td>\n",
       "      <td>Rice</td>\n",
       "      <td>5434696.0</td>\n",
       "      <td>58322509.35</td>\n",
       "      <td>10731.51</td>\n",
       "      <td>211.64</td>\n",
       "      <td>61.4</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   production_id  country_id  year      crop  area_harvested_ha  \\\n",
       "0              1           6  2001  Soybeans          3751494.0   \n",
       "1              2          20  1993     Maize          2112762.0   \n",
       "2              3          26  1995  Soybeans          1650777.0   \n",
       "3              4          33  2018     Wheat          4782989.0   \n",
       "4              5          13  2013      Rice          5434696.0   \n",
       "\n",
       "   production_tonnes  yield_kg_ha  fertilizer_use_kg_ha  irrigation_pct notes  \n",
       "0        12036421.75      3208.43                100.90             NaN  None  \n",
       "1        11377270.55      5385.02                 19.14             9.8  None  \n",
       "2         7474101.16      4527.63                193.84            56.6  None  \n",
       "3        32397951.41      6773.58                205.12            62.5  None  \n",
       "4        58322509.35     10731.51                211.64            61.4  None  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CROP PRODUCTION TABLE\")\n",
    "print(\"=\" * 60)\n",
    "df_crops.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEMPERATURE ANOMALIES TABLE\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anomaly_id</th>\n",
       "      <th>country_id</th>\n",
       "      <th>year</th>\n",
       "      <th>annual_anomaly_c</th>\n",
       "      <th>jan</th>\n",
       "      <th>feb</th>\n",
       "      <th>mar</th>\n",
       "      <th>apr</th>\n",
       "      <th>may</th>\n",
       "      <th>jun</th>\n",
       "      <th>jul</th>\n",
       "      <th>aug</th>\n",
       "      <th>sep</th>\n",
       "      <th>oct</th>\n",
       "      <th>nov</th>\n",
       "      <th>dec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>1990</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.44</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>1991</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>1992</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>1993</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.55</td>\n",
       "      <td>-0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>1994</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.71</td>\n",
       "      <td>1.27</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   anomaly_id  country_id  year  annual_anomaly_c   jan   feb   mar   apr  \\\n",
       "0           1          33  1990              0.07 -0.02   NaN   NaN -0.09   \n",
       "1           2          33  1991              0.20  0.36  0.40  0.60  0.74   \n",
       "2           3          33  1992              0.54  0.46  0.76  0.85  0.68   \n",
       "3           4          33  1993              0.43  0.28  0.22  0.74  0.60   \n",
       "4           5          33  1994              0.87  0.75  0.86  0.65  1.14   \n",
       "\n",
       "    may   jun   jul   aug   sep   oct   nov   dec  \n",
       "0 -0.11  0.44 -0.44  0.30 -0.08 -0.03 -0.31 -0.39  \n",
       "1  0.22  0.34  0.22  0.70   NaN -0.44 -0.50 -0.22  \n",
       "2  0.60  0.98  0.52  0.92  0.53  0.01  0.09  0.07  \n",
       "3   NaN  1.30  0.81  0.35 -0.30  0.04  0.55 -0.13  \n",
       "4  0.45  1.71  1.27  1.04  0.01  0.62  1.05  0.88  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TEMPERATURE ANOMALIES TABLE\")\n",
    "print(\"=\" * 60)\n",
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Check and Document Data Types\n",
    "\n",
    "**YOUR TASK:** \n",
    "1. Use `.dtypes` to check the data types of each DataFrame\n",
    "2. Use `.info()` to get a summary including non-null counts\n",
    "3. Answer the questions in the markdown cell below\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# YOUR CODE HERE: Check data types for df_crops\n",
    "# ============================================\n",
    "\n",
    "# Print the data types\n",
    "\n",
    "\n",
    "# Print info (includes non-null counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# YOUR CODE HERE: Check data types for df_temp\n",
    "# ============================================\n",
    "\n",
    "# Print the data types\n",
    "\n",
    "\n",
    "# Print info\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Answer These Questions (write your answers below)\n",
    "\n",
    "1. What is the data type of the `year` column in `df_crops`?\n",
    "\n",
    "   **Your answer:** \n",
    "\n",
    "2. What is the data type of the `yield_kg_ha` column? Why is it this type?\n",
    "\n",
    "   **Your answer:** \n",
    "\n",
    "3. How many monthly temperature columns are there in `df_temp`? List them.\n",
    "\n",
    "   **Your answer:** \n",
    "\n",
    "4. Which column in `df_crops` has the most NULL values? How many?\n",
    "\n",
    "   **Your answer:** \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Question 2: Reshape Data - Wide to Long (25 points)\n",
    "\n",
    "## Task: Convert Monthly Temperature Data from Wide to Long Format\n",
    "\n",
    "The `temperature_anomalies` table has **monthly data stored in wide format** (columns: jan, feb, mar, ..., dec). This is a classic case where we need to reshape the data to make it **tidy**.\n",
    "\n",
    "### Tidy Data Principle\n",
    "- Each **variable** should have its own **column**\n",
    "- Each **observation** should have its own **row**\n",
    "- Each **value** should have its own **cell**\n",
    "\n",
    "### Current Structure (Wide - NOT Tidy)\n",
    "```\n",
    "country_id | year | annual_anomaly_c | jan  | feb  | mar  | ... | dec\n",
    "-----------+------+------------------+------+------+------+-----+-----\n",
    "    1      | 2020 |      1.5         | 1.2  | 1.8  | 1.4  | ... | 1.6\n",
    "```\n",
    "\n",
    "### Target Structure (Long - Tidy)\n",
    "```\n",
    "country_id | year | month | monthly_anomaly_c\n",
    "-----------+------+-------+------------------\n",
    "    1      | 2020 |  jan  |       1.2\n",
    "    1      | 2020 |  feb  |       1.8\n",
    "    1      | 2020 |  mar  |       1.4\n",
    "    ...    | ...  |  ...  |       ...\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's look at the current structure\n",
    "print(\"Current temperature table structure:\")\n",
    "print(f\"Shape: {df_temp.shape}\")\n",
    "print(f\"\\nColumns: {df_temp.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Use pd.melt() to Reshape the Data\n",
    "\n",
    "**YOUR TASK:** Complete the code below to:\n",
    "1. Use `pd.melt()` to convert monthly columns to rows\n",
    "2. Keep `country_id`, `year`, and `annual_anomaly_c` as identifier columns\n",
    "3. Melt the month columns (jan, feb, mar, apr, may, jun, jul, aug, sep, oct, nov, dec)\n",
    "4. Name the new columns: `month` and `monthly_anomaly_c`\n",
    "\n",
    "**Hint:** Use the syntax:\n",
    "```python\n",
    "pd.melt(df, \n",
    "        id_vars=['cols', 'to', 'keep'], \n",
    "        value_vars=['cols', 'to', 'unpivot'],\n",
    "        var_name='new_col_name', \n",
    "        value_name='value_col_name')\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# YOUR CODE HERE: Reshape temperature data\n",
    "# ============================================\n",
    "\n",
    "# Define the month columns to unpivot\n",
    "month_columns = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', \n",
    "                 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
    "\n",
    "# Use pd.melt() to reshape from wide to long\n",
    "df_temp_long = pd.melt(\n",
    "    df_temp,\n",
    "    id_vars=___,  # columns to keep as identifiers\n",
    "    value_vars=___,  # columns to unpivot\n",
    "    var_name=___,  # name for the new 'month' column\n",
    "    value_name=___  # name for the new value column\n",
    ")\n",
    "\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the reshape\n",
    "print(\"Reshaped temperature data:\")\n",
    "print(f\"Original shape: {df_temp.shape}\")\n",
    "print(f\"New shape: {df_temp_long.shape}\")\n",
    "print(f\"\\nExpected rows: {len(df_temp)} Ã— 12 months = {len(df_temp) * 12}\")\n",
    "print(f\"Actual rows: {len(df_temp_long)}\")\n",
    "\n",
    "print(\"\\nSample of reshaped data:\")\n",
    "df_temp_long.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Add Month Number for Sorting\n",
    "\n",
    "**YOUR TASK:** Create a `month_num` column that converts month names to numbers (jan=1, feb=2, ..., dec=12)\n",
    "\n",
    "**Hint:** Create a dictionary mapping and use `.map()`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# YOUR CODE HERE: Add month number column\n",
    "# ============================================\n",
    "\n",
    "# Create a mapping dictionary\n",
    "month_to_num = {\n",
    "    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4,\n",
    "    'may': 5, 'jun': 6, 'jul': 7, 'aug': 8,\n",
    "    'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "}\n",
    "\n",
    "# Add the month_num column using .map()\n",
    "df_temp_long['month_num'] = ___\n",
    "\n",
    "# Sort by country_id, year, month_num\n",
    "df_temp_long = df_temp_long.sort_values(['country_id', 'year', 'month_num'])\n",
    "\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the month numbers\n",
    "print(\"Temperature data with month numbers:\")\n",
    "df_temp_long[['country_id', 'year', 'month', 'month_num', 'monthly_anomaly_c']].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Answer These Questions\n",
    "\n",
    "1. Why is the long format considered \"tidy\" for this data?\n",
    "\n",
    "   **Your answer:** \n",
    "\n",
    "2. What is the formula to calculate the expected number of rows after melting?\n",
    "\n",
    "   **Your answer:** \n",
    "\n",
    "3. When would you want to convert BACK from long to wide format?\n",
    "\n",
    "   **Your answer:** \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Question 3: Missing Data Analysis (25 points)\n",
    "\n",
    "## Task: Analyze and Handle Missing Data\n",
    "\n",
    "As discussed in Lecture 6, understanding **why** data is missing helps decide how to handle it:\n",
    "\n",
    "| Type | Description | Strategy |\n",
    "|------|-------------|----------|\n",
    "| **MCAR** | Missing Completely At Random | Drop or impute |\n",
    "| **MAR** | Missing At Random (depends on other variables) | Impute with care |\n",
    "| **MNAR** | Missing Not At Random | Cannot ignore |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Calculate Missing Data Statistics\n",
    "\n",
    "**YOUR TASK:** Create a function that calculates missing data statistics for any DataFrame.\n",
    "\n",
    "The function should return a DataFrame with:\n",
    "- Column name\n",
    "- Total count\n",
    "- Missing count\n",
    "- Missing percentage\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# YOUR CODE HERE: Create missing data function\n",
    "# ============================================\n",
    "\n",
    "def missing_data_report(df):\n",
    "    \"\"\"\n",
    "    Generate a missing data report for a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame to analyze\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame with columns: column, total, missing, missing_pct\n",
    "    \"\"\"\n",
    "    # Calculate total rows\n",
    "    total = len(df)\n",
    "    \n",
    "    # Calculate missing for each column\n",
    "    # Hint: use df.isnull().sum() for missing count\n",
    "    \n",
    "    report = pd.DataFrame({\n",
    "        'column': ___,\n",
    "        'total': ___,\n",
    "        'missing': ___,\n",
    "        'missing_pct': ___\n",
    "    })\n",
    "    \n",
    "    # Round percentage to 2 decimal places\n",
    "    report['missing_pct'] = report['missing_pct'].round(2)\n",
    "    \n",
    "    # Sort by missing_pct descending\n",
    "    report = report.sort_values('missing_pct', ascending=False)\n",
    "    \n",
    "    return report\n",
    "\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function on crop_production\n",
    "print(\"Missing Data Report: Crop Production\")\n",
    "print(\"=\" * 50)\n",
    "missing_crops = missing_data_report(df_crops)\n",
    "missing_crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on temperature (long format)\n",
    "print(\"Missing Data Report: Temperature Anomalies (Long)\")\n",
    "print(\"=\" * 50)\n",
    "missing_temp = missing_data_report(df_temp_long)\n",
    "missing_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Analyze Missing Patterns\n",
    "\n",
    "**YOUR TASK:** Investigate which countries/years have the most missing temperature data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# YOUR CODE HERE: Find countries with most missing monthly data\n",
    "# ============================================\n",
    "\n",
    "# Group by country_id and count missing values\n",
    "# Hint: use .isnull() and .sum() after groupby\n",
    "\n",
    "missing_by_country = df_temp_long.groupby('country_id')['monthly_anomaly_c'].apply(\n",
    "    lambda x: ___  # count null values\n",
    ").reset_index()\n",
    "\n",
    "missing_by_country.columns = ['country_id', 'missing_months']\n",
    "\n",
    "# Sort by missing count descending and show top 10\n",
    "missing_by_country = missing_by_country.sort_values('missing_months', ascending=False)\n",
    "print(\"Countries with most missing monthly temperature data:\")\n",
    "missing_by_country.head(10)\n",
    "\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Handle Missing Data\n",
    "\n",
    "**YOUR TASK:** For the `df_crops` DataFrame, handle missing `yield_kg_ha` values using **group-based imputation** (fill with the mean yield for each crop type).\n",
    "\n",
    "This is appropriate when we believe the missing mechanism is **MAR** - missing values depend on the crop type.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# YOUR CODE HERE: Impute missing yield with crop-specific mean\n",
    "# ============================================\n",
    "\n",
    "# Make a copy to avoid modifying original\n",
    "df_crops_imputed = df_crops.copy()\n",
    "\n",
    "# Count missing before\n",
    "missing_before = df_crops_imputed['yield_kg_ha'].isnull().sum()\n",
    "print(f\"Missing yield values before: {missing_before}\")\n",
    "\n",
    "# Calculate mean yield for each crop\n",
    "crop_mean_yield = df_crops_imputed.groupby('crop')['yield_kg_ha'].transform('mean')\n",
    "\n",
    "# Fill missing values with crop-specific mean\n",
    "# Hint: use fillna()\n",
    "df_crops_imputed['yield_kg_ha'] = df_crops_imputed['yield_kg_ha'].fillna(___)\n",
    "\n",
    "# Count missing after\n",
    "missing_after = df_crops_imputed['yield_kg_ha'].isnull().sum()\n",
    "print(f\"Missing yield values after: {missing_after}\")\n",
    "\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a flag column to track which values were imputed\n",
    "df_crops_imputed['yield_imputed'] = df_crops['yield_kg_ha'].isnull()\n",
    "\n",
    "print(f\"Rows with imputed yield: {df_crops_imputed['yield_imputed'].sum()}\")\n",
    "\n",
    "# Show some imputed rows\n",
    "print(\"\\nSample of imputed rows:\")\n",
    "df_crops_imputed[df_crops_imputed['yield_imputed']][['country_id', 'year', 'crop', 'yield_kg_ha', 'yield_imputed']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D: Answer These Questions\n",
    "\n",
    "1. What percentage of `yield_kg_ha` values were missing in the original data?\n",
    "\n",
    "   **Your answer:** \n",
    "\n",
    "2. Why did we use crop-specific mean instead of overall mean for imputation?\n",
    "\n",
    "   **Your answer:** \n",
    "\n",
    "3. Why is it important to create a flag column (`yield_imputed`) to track imputed values?\n",
    "\n",
    "   **Your answer:** \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Question 4: Data Validation (25 points)\n",
    "\n",
    "## Task: Implement Validation Checks\n",
    "\n",
    "As discussed in Lecture 6, validation catches problems early:\n",
    "\n",
    "| Type | Check | Example |\n",
    "|------|-------|----------|\n",
    "| **Range** | Values within bounds | Year between 1900-2100 |\n",
    "| **Null** | Required fields present | country_id not null |\n",
    "| **Type** | Correct data type | Year is integer |\n",
    "| **Uniqueness** | No duplicates | Unique country-year-crop combo |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Range Validation\n",
    "\n",
    "**YOUR TASK:** Check that values are within expected ranges:\n",
    "- `year`: between 1900 and 2100\n",
    "- `yield_kg_ha`: between 0 and 50000 (reasonable crop yield)\n",
    "- `irrigation_pct`: between 0 and 100\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# YOUR CODE HERE: Implement range validation\n",
    "# ============================================\n",
    "\n",
    "def validate_range(df, column, min_val, max_val):\n",
    "    \"\"\"\n",
    "    Check if values in a column are within the specified range.\n",
    "    \n",
    "    Returns: DataFrame with rows that FAIL validation\n",
    "    \"\"\"\n",
    "    # Find rows where value is outside range (excluding nulls)\n",
    "    # Hint: use (df[column] < min_val) | (df[column] > max_val)\n",
    "    \n",
    "    mask = ___\n",
    "    \n",
    "    invalid_rows = df[mask]\n",
    "    return invalid_rows\n",
    "\n",
    "# ============================================\n",
    "\n",
    "# Test range validations\n",
    "print(\"Range Validation Results\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check year range\n",
    "invalid_year = validate_range(df_crops, 'year', 1900, 2100)\n",
    "print(f\"Invalid year values (outside 1900-2100): {len(invalid_year)}\")\n",
    "\n",
    "# Check yield range\n",
    "invalid_yield = validate_range(df_crops, 'yield_kg_ha', 0, 50000)\n",
    "print(f\"Invalid yield values (outside 0-50000): {len(invalid_yield)}\")\n",
    "\n",
    "# Check irrigation range\n",
    "invalid_irrigation = validate_range(df_crops, 'irrigation_pct', 0, 100)\n",
    "print(f\"Invalid irrigation values (outside 0-100): {len(invalid_irrigation)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Null Validation\n",
    "\n",
    "**YOUR TASK:** Check that required fields are not null:\n",
    "- `country_id`: Required\n",
    "- `year`: Required\n",
    "- `crop`: Required\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# YOUR CODE HERE: Implement null validation\n",
    "# ============================================\n",
    "\n",
    "def validate_not_null(df, columns):\n",
    "    \"\"\"\n",
    "    Check that specified columns have no null values.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "    columns : list of column names\n",
    "    \n",
    "    Returns: dict with column name and count of nulls\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for col in columns:\n",
    "        # Count null values in each column\n",
    "        null_count = ___\n",
    "        results[col] = null_count\n",
    "    return results\n",
    "\n",
    "# ============================================\n",
    "\n",
    "# Test null validation\n",
    "required_columns = ['country_id', 'year', 'crop']\n",
    "null_results = validate_not_null(df_crops, required_columns)\n",
    "\n",
    "print(\"Null Validation Results\")\n",
    "print(\"=\" * 50)\n",
    "for col, count in null_results.items():\n",
    "    status = \"PASS\" if count == 0 else \"FAIL\"\n",
    "    print(f\"{col}: {count} nulls [{status}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Uniqueness Validation\n",
    "\n",
    "**YOUR TASK:** Check that there are no duplicate records for the same country-year-crop combination.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# YOUR CODE HERE: Check for duplicates\n",
    "# ============================================\n",
    "\n",
    "def validate_unique(df, columns):\n",
    "    \"\"\"\n",
    "    Check that combinations of columns are unique.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "    columns : list of columns that should be unique together\n",
    "    \n",
    "    Returns: DataFrame with duplicate rows\n",
    "    \"\"\"\n",
    "    # Find duplicates\n",
    "    # Hint: use df.duplicated(subset=columns, keep=False)\n",
    "    \n",
    "    duplicates = df[___]\n",
    "    return duplicates\n",
    "\n",
    "# ============================================\n",
    "\n",
    "# Test uniqueness validation\n",
    "unique_cols = ['country_id', 'year', 'crop']\n",
    "duplicates = validate_unique(df_crops, unique_cols)\n",
    "\n",
    "print(\"Uniqueness Validation Results\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Duplicate country-year-crop combinations: {len(duplicates)}\")\n",
    "\n",
    "if len(duplicates) > 0:\n",
    "    print(\"\\nSample duplicates:\")\n",
    "    print(duplicates[unique_cols + ['production_tonnes']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D: Create a Complete Validation Report\n",
    "\n",
    "**YOUR TASK:** Combine all validations into a summary report.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# YOUR CODE HERE: Create validation summary\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA VALIDATION SUMMARY - CROP PRODUCTION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total rows: {len(df_crops)}\")\n",
    "print(f\"Generated at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print()\n",
    "\n",
    "# 1. Range checks\n",
    "print(\"1. RANGE CHECKS\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"   Year (1900-2100): {len(validate_range(df_crops, 'year', 1900, 2100))} failures\")\n",
    "print(f\"   Yield (0-50000): {len(validate_range(df_crops, 'yield_kg_ha', 0, 50000))} failures\")\n",
    "print(f\"   Irrigation (0-100): {len(validate_range(df_crops, 'irrigation_pct', 0, 100))} failures\")\n",
    "print()\n",
    "\n",
    "# 2. Null checks\n",
    "print(\"2. NULL CHECKS (Required Fields)\")\n",
    "print(\"-\" * 40)\n",
    "for col, count in validate_not_null(df_crops, ['country_id', 'year', 'crop']).items():\n",
    "    status = \"PASS\" if count == 0 else f\"FAIL ({count} nulls)\"\n",
    "    print(f\"   {col}: {status}\")\n",
    "print()\n",
    "\n",
    "# 3. Uniqueness check\n",
    "print(\"3. UNIQUENESS CHECK\")\n",
    "print(\"-\" * 40)\n",
    "dup_count = len(validate_unique(df_crops, ['country_id', 'year', 'crop']))\n",
    "status = \"PASS\" if dup_count == 0 else f\"FAIL ({dup_count} duplicates)\"\n",
    "print(f\"   country_id + year + crop: {status}\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part E: Answer These Questions\n",
    "\n",
    "1. Why is range validation important for data quality?\n",
    "\n",
    "   **Your answer:** \n",
    "\n",
    "2. What would you do if you found duplicate country-year-crop records?\n",
    "\n",
    "   **Your answer:** \n",
    "\n",
    "3. List one additional validation check that would be useful for this dataset.\n",
    "\n",
    "   **Your answer:** \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Submission Checklist\n",
    "\n",
    "Before submitting, make sure:\n",
    "\n",
    "- [ ] **Question 1**: Checked data types and answered all questions\n",
    "- [ ] **Question 2**: Successfully reshaped temperature data from wide to long\n",
    "- [ ] **Question 3**: Created missing data report and implemented imputation\n",
    "- [ ] **Question 4**: Implemented all three validation checks (range, null, unique)\n",
    "- [ ] All markdown questions have been answered\n",
    "\n",
    "### How to Submit\n",
    "\n",
    "1. Save this notebook\n",
    "2. Export as PDF or HTML\n",
    "3. Submit via Canvas by **Wednesday, January 21, 2026 at 11:59 PM**\n",
    "\n",
    "---\n",
    "\n",
    "## Grading Rubric\n",
    "\n",
    "| Question | Points | Description |\n",
    "|----------|--------|-------------|\n",
    "| Q1 | 15 | Data type inspection and questions |\n",
    "| Q2 | 25 | Reshape (melt) implementation |\n",
    "| Q3 | 25 | Missing data analysis and imputation |\n",
    "| Q4 | 25 | Validation checks implementation |\n",
    "| **Style** | 10 | Code quality, comments, formatting |\n",
    "| **Total** | **100** | |\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ubc-mfre",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
