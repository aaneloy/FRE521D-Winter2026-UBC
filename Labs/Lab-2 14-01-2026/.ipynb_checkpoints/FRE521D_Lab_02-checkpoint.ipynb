{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FRE 521D: Data Analytics in Climate, Food and Environment\n",
    "## Lab 2: Building an ETL Pipeline\n",
    "\n",
    "**Program:** UBC Master of Food and Resource Economics  \n",
    "**Instructor:** Asif Ahmed Neloy\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"background-color: #FFF3CD; border-left: 4px solid #E6A23C; padding: 15px; margin: 15px 0;\">\n",
    "    <h3 style=\"margin-top: 0; color: #856404;\">Submission Deadline</h3>\n",
    "    <p style=\"margin-bottom: 0; font-size: 1.2em;\"><strong>End of Day: Wednesday, January 14, 2026</strong></p>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## Lab Objectives\n",
    "\n",
    "In this lab, you will build an ETL (Extract, Transform, Load) pipeline for climate change data. You will:\n",
    "\n",
    "1. **Extract**: Load the raw CSV data safely and add metadata columns\n",
    "2. **Transform**: Clean column names and select relevant columns\n",
    "3. **Load**: Save both raw and cleaned layers with proper documentation\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "The `climate_change_indicators.csv` file contains **surface temperature change** data from the FAO (Food and Agriculture Organization). It shows how much warmer or cooler each country was compared to the 1951-1980 baseline period.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries and Create Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 15)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Create directory structure for raw and cleaned layers\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "os.makedirs('data/cleaned', exist_ok=True)\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Raw Data First\n",
    "\n",
    "As covered in Lecture 3, **always inspect your data before loading it**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first few lines as raw text\n",
    "with open('climate_change_indicators.csv', 'r', encoding='utf-8-sig') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i < 3:\n",
    "            print(f\"Line {i}: {line[:100]}...\")\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore the data\n",
    "df = pd.read_csv('climate_change_indicators.csv', encoding='utf-8-sig')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "print(f\"\\nColumn names:\")\n",
    "for i, col in enumerate(df.columns):\n",
    "    print(f\"  {i}: {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View sample of the data\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Question 1: Extract Phase - Create Raw Layer \n",
    "## Task: Extract Data and Add Metadata Columns\n",
    "\n",
    "As discussed in Lecture 3, the **raw layer** should:\n",
    "1. Contain the exact source data (no transformations)\n",
    "2. Add metadata columns to track data lineage\n",
    "\n",
    "**Complete the function below** to add these metadata columns:\n",
    "- `_source_file`: The name of the source file (use `os.path.basename()`)\n",
    "- `_extracted_at`: The timestamp when extracted (use `datetime.now().isoformat()`)\n",
    "- `_row_num`: Row numbers starting from 1 (use `range(1, len(df) + 1)`)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_to_raw(filepath):\n",
    "    \"\"\"\n",
    "    Extract data from source file and add metadata columns.\n",
    "    This creates the RAW LAYER of our ETL pipeline.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    filepath : str\n",
    "        Path to the source CSV file\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame with source data plus metadata columns\n",
    "    \"\"\"\n",
    "    # Read the CSV file (keeping data as-is)\n",
    "    df = pd.read_csv(filepath, encoding='utf-8-sig')\n",
    "    \n",
    "    # ============================================\n",
    "    # YOUR CODE HERE: Add the 3 metadata columns\n",
    "    # ============================================\n",
    "    \n",
    "    # 1. Add _source_file column\n",
    "    \n",
    "    \n",
    "    # 2. Add _extracted_at column\n",
    "    \n",
    "    \n",
    "    # 3. Add _row_num column\n",
    "    \n",
    "    \n",
    "    # ============================================\n",
    "    \n",
    "    print(f\"Extracted {len(df)} rows from {filepath}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function\n",
    "df_raw = extract_to_raw('climate_change_indicators.csv')\n",
    "\n",
    "# Verify metadata columns were added\n",
    "print(\"\\nChecking metadata columns:\")\n",
    "print(f\"  _source_file exists: {'_source_file' in df_raw.columns}\")\n",
    "print(f\"  _extracted_at exists: {'_extracted_at' in df_raw.columns}\")\n",
    "print(f\"  _row_num exists: {'_row_num' in df_raw.columns}\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample with metadata columns:\")\n",
    "df_raw[['Country', 'ISO3', '_source_file', '_extracted_at', '_row_num']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the raw layer\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "raw_filepath = f'data/raw/climate_indicators_raw_{timestamp}.csv'\n",
    "df_raw.to_csv(raw_filepath, index=False)\n",
    "print(f\"Raw layer saved to: {raw_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Question 2: Transform Phase - Create Cleaned Layer\n",
    "\n",
    "## Task: Clean and Simplify the Data\n",
    "\n",
    "The raw data has many columns we don't need. For the **cleaned layer**, we will:\n",
    "1. Select only the useful columns\n",
    "2. Rename columns to be more database-friendly (lowercase, underscores)\n",
    "3. Add a timestamp for when the data was cleaned\n",
    "\n",
    "**Complete the function below** to:\n",
    "1. Select these columns: `Country`, `ISO3`, `F2000`, `F2010`, `F2020`, `F2022`\n",
    "2. Rename them to: `country`, `iso3`, `temp_2000`, `temp_2010`, `temp_2020`, `temp_2022`\n",
    "3. Add a `_cleaned_at` column with the current timestamp\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_cleaned(df_raw):\n",
    "    \"\"\"\n",
    "    Transform raw data to cleaned layer.\n",
    "    Selects relevant columns and standardizes names.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_raw : pd.DataFrame\n",
    "        Raw data from extract phase\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame with cleaned data\n",
    "    \"\"\"\n",
    "    # ============================================\n",
    "    # YOUR CODE HERE\n",
    "    # ============================================\n",
    "    \n",
    "    # Step 1: Select only the columns we need\n",
    "    # Columns to keep: 'Country', 'ISO3', 'F2000', 'F2010', 'F2020', 'F2022'\n",
    "    columns_to_keep = ['Country', 'ISO3', 'F2000', 'F2010', 'F2020', 'F2022']\n",
    "    df = df_raw[columns_to_keep].copy()\n",
    "    \n",
    "    # Step 2: Rename columns to be database-friendly\n",
    "    # Use df.rename(columns={old_name: new_name, ...})\n",
    "    # New names: 'country', 'iso3', 'temp_2000', 'temp_2010', 'temp_2020', 'temp_2022'\n",
    "    \n",
    "    df = df.rename(columns={\n",
    "        # YOUR CODE: fill in the mapping\n",
    "        \n",
    "    })\n",
    "    \n",
    "    # Step 3: Add _cleaned_at timestamp column\n",
    "    \n",
    "    \n",
    "    # ============================================\n",
    "    \n",
    "    print(f\"Cleaned data: {len(df)} rows, {len(df.columns)} columns\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function\n",
    "df_clean = transform_to_cleaned(df_raw)\n",
    "\n",
    "# Verify the transformation\n",
    "print(\"\\nCleaned column names:\")\n",
    "print(df_clean.columns.tolist())\n",
    "\n",
    "print(\"\\nSample of cleaned data:\")\n",
    "df_clean.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned layer\n",
    "cleaned_filepath = f'data/cleaned/climate_indicators_cleaned_{timestamp}.csv'\n",
    "df_clean.to_csv(cleaned_filepath, index=False)\n",
    "print(f\"Cleaned layer saved to: {cleaned_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Question 3: Data Lineage Documentation \n",
    "\n",
    "## Task: Document Your Pipeline\n",
    "\n",
    "As discussed in Lecture 3, **data lineage** tracks:\n",
    "- Where the data came from\n",
    "- What transformations were applied\n",
    "- When the pipeline was run\n",
    "\n",
    "**Fill in the transformations list** with at least 4 things your pipeline did.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# YOUR CODE HERE: List at least 4 transformations\n",
    "# ============================================\n",
    "\n",
    "transformations = [\n",
    "    # Example: \"Added metadata columns (_source_file, _extracted_at, _row_num)\"\n",
    "    # List what your pipeline did...\n",
    "    \n",
    "]\n",
    "\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate lineage documentation\n",
    "lineage_doc = f\"\"\"\n",
    "# Data Lineage Documentation\n",
    "\n",
    "Generated: {datetime.now().isoformat()}\n",
    "\n",
    "## Source\n",
    "- File: climate_change_indicators.csv\n",
    "- Description: FAO Climate Change Indicators - Surface Temperature Change\n",
    "- Rows: {len(df_raw)}\n",
    "\n",
    "## Raw Layer\n",
    "- File: {raw_filepath}\n",
    "- Contains exact copy of source with metadata columns added\n",
    "\n",
    "## Cleaned Layer\n",
    "- File: {cleaned_filepath}\n",
    "- Columns: {df_clean.columns.tolist()}\n",
    "\n",
    "## Transformations Applied\n",
    "\"\"\"\n",
    "\n",
    "for i, t in enumerate(transformations, 1):\n",
    "    lineage_doc += f\"{i}. {t}\\n\"\n",
    "\n",
    "# Save and display\n",
    "with open('data/cleaned/README.md', 'w') as f:\n",
    "    f.write(lineage_doc)\n",
    "\n",
    "print(lineage_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Verify Your Work\n",
    "\n",
    "Run this cell to check that everything was created correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick verification\n",
    "print(\"=\" * 50)\n",
    "print(\"ETL Pipeline Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\n[EXTRACT] Raw layer:\")\n",
    "print(f\"  - Rows: {len(df_raw)}\")\n",
    "print(f\"  - Has _source_file: {'_source_file' in df_raw.columns}\")\n",
    "print(f\"  - Has _extracted_at: {'_extracted_at' in df_raw.columns}\")\n",
    "print(f\"  - Has _row_num: {'_row_num' in df_raw.columns}\")\n",
    "\n",
    "print(f\"\\n[TRANSFORM] Cleaned layer:\")\n",
    "print(f\"  - Rows: {len(df_clean)}\")\n",
    "print(f\"  - Columns: {df_clean.columns.tolist()}\")\n",
    "\n",
    "print(f\"\\n[LOAD] Files saved:\")\n",
    "print(f\"  - Raw: {raw_filepath}\")\n",
    "print(f\"  - Cleaned: {cleaned_filepath}\")\n",
    "print(f\"  - README: data/cleaned/README.md\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Submission Checklist\n",
    "\n",
    "Before submitting, make sure:\n",
    "\n",
    "- [ ] **Question 1**: `extract_to_raw()` adds all 3 metadata columns\n",
    "- [ ] **Question 2**: `transform_to_cleaned()` selects and renames columns correctly\n",
    "- [ ] **Question 3**: You listed at least 4 transformations\n",
    "- [ ] All files were saved to `data/raw/` and `data/cleaned/`\n",
    "\n",
    "### How to Submit\n",
    "\n",
    "1. Save this notebook\n",
    "2. Submit via Canvas (Lab 2 Submission) by **end of day Wednesday, January 14, 2026**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
