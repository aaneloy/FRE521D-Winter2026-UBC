{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FRE 521D: Data Analytics in Climate, Food and Environment\n",
    "## Lecture 3: ETL Pipeline I - Extracting from Files and Tables\n",
    "\n",
    "**Date:** Monday, January 12, 2026  \n",
    "**Instructor:** Asif Ahmed Neloy  \n",
    "**Program:** UBC Master of Food and Resource Economics\n",
    "\n",
    "---\n",
    "\n",
    "### Today's Agenda\n",
    "\n",
    "1. What is ETL? Why Do We Need It?\n",
    "2. The Extract Phase: Getting Data from Files\n",
    "3. Working with CSV Files (The Right Way)\n",
    "4. Working with JSON Files\n",
    "5. Raw Layer vs Cleaned Layer\n",
    "6. Data Lineage: Where Did This Data Come From?\n",
    "7. Hands-on Practice\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is ETL? Why Do We Need It?\n",
    "\n",
    "### The Problem We Face\n",
    "\n",
    "In Assignment 1, you loaded CSV files into MySQL. That worked fine for a one-time load. But in the real world:\n",
    "\n",
    "- Weather data updates **every day**\n",
    "- Commodity prices change **every hour**\n",
    "- ESG ratings are revised **quarterly**\n",
    "- Sensor data arrives **every minute**\n",
    "\n",
    "Manual loading does not work when data keeps coming. We need **automated pipelines**.\n",
    "\n",
    "### ETL: The Three Steps\n",
    "\n",
    "**ETL** stands for **Extract, Transform, Load**:\n",
    "\n",
    "```\n",
    "┌────────────────┐     ┌────────────────┐     ┌────────────────┐\n",
    "│    EXTRACT     │ --> │   TRANSFORM    │ --> │      LOAD      │\n",
    "│                │     │                │     │                │\n",
    "│ Pull data from │     │ Clean, reshape │     │ Store in final │\n",
    "│ source systems │     │ validate data  │     │ destination    │\n",
    "└────────────────┘     └────────────────┘     └────────────────┘\n",
    "```\n",
    "\n",
    "| Step | What Happens | Examples |\n",
    "|------|--------------|----------|\n",
    "| **Extract** | Pull data from source | Read CSV, call API, query database |\n",
    "| **Transform** | Clean and reshape | Fix data types, handle nulls, join tables |\n",
    "| **Load** | Store in destination | Insert into database, write to file |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why ETL Matters for Your Career\n",
    "\n",
    "Data analysts spend about **80% of their time** on data preparation. If you can build reliable ETL pipelines, you will:\n",
    "\n",
    "1. **Save time** - Automate repetitive tasks\n",
    "2. **Reduce errors** - Consistent processing every time\n",
    "3. **Enable analysis** - Clean data is ready for insights\n",
    "4. **Stand out** - Many analysts cannot do this\n",
    "\n",
    "In Assignment 2, you will build a complete ETL pipeline that pulls weather data from an API and loads it into your database from Assignment 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Setting Up Our Environment\n",
    "\n",
    "Let's install and import what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\anaconda3\\envs\\gpu\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy in c:\\anaconda3\\envs\\gpu\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: requests in c:\\anaconda3\\envs\\gpu\\lib\\site-packages (2.32.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\anaconda3\\envs\\gpu\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\anaconda3\\envs\\gpu\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\anaconda3\\envs\\gpu\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\anaconda3\\envs\\gpu\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda3\\envs\\gpu\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\anaconda3\\envs\\gpu\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda3\\envs\\gpu\\lib\\site-packages (from requests) (2025.7.14)\n",
      "Requirement already satisfied: six>=1.5 in c:\\anaconda3\\envs\\gpu\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Run this cell once to install required packages\n",
    "# Remove the # to uncomment if you need to install\n",
    "\n",
    "!pip install pandas numpy requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 2.3.1\n",
      "Current time: 2026-01-14 09:28:23\n",
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Standard imports for ETL work\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Display settings for better output\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Working with CSV Files (The Right Way)\n",
    "\n",
    "CSV looks simple but has many traps. Let's learn to handle them.\n",
    "\n",
    "### Common CSV Problems\n",
    "\n",
    "| Problem | What It Looks Like | Solution |\n",
    "|---------|-------------------|----------|\n",
    "| European decimals | `1.234,56` instead of `1234.56` | `decimal=','` |\n",
    "| Semicolon delimiter | Columns separated by `;` | `sep=';'` |\n",
    "| Encoding issues | `Ã©` instead of `é` | `encoding='utf-8'` |\n",
    "| Missing values | `NA`, `..`, `-`, blank | `na_values` parameter |\n",
    "| Mixed types | Numbers stored as text | Read as string first |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 0: Country,ISO3_Code,Region,Income_Group,Year,Crop,Area_Harvested_Ha,Production_Tonnes,Yield_Kg_Ha,Fert...\n",
      "Line 1: China,CHN,East Asia,Upper middle income,2001,Soybeans,3751494,12036421.75,3208.43,100.9,,...\n",
      "Line 2: Nepal,NPL,South Asia,Low income,1993,Maize,2112762,11377270.55,\"5385,02\",\"19,14\",9.8,...\n",
      "Line 3: South Korea,KOR,East Asia,High income,1995.0,Soybeans,1650777,7474101.16,4527.63,193.84,56.6,...\n",
      "Line 4: United States,USA,North America,High income,2018,Wheat,4782989,32397951.41,6773.58,205.12,62.5,...\n"
     ]
    }
   ],
   "source": [
    "# Let's look at our crop data file first\n",
    "# Always inspect before loading!\n",
    "\n",
    "# Read just the first few lines as text\n",
    "with open('crop_production_1990_2023.csv', 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i < 5:  # First 5 lines\n",
    "            print(f\"Line {i}: {line.strip()[:100]}...\")  # First 100 chars\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: 4187 rows, 12 columns\n",
      "\n",
      "Column names:\n",
      "['Country', 'ISO3_Code', 'Region', 'Income_Group', 'Year', 'Crop', 'Area_Harvested_Ha', 'Production_Tonnes', 'Yield_Kg_Ha', 'Fertilizer_Use_Kg_Ha', 'Irrigation_Pct', 'Notes']\n",
      "\n",
      "Data types:\n",
      "Country                  object\n",
      "ISO3_Code                object\n",
      "Region                   object\n",
      "Income_Group             object\n",
      "Year                    float64\n",
      "Crop                     object\n",
      "Area_Harvested_Ha         int64\n",
      "Production_Tonnes        object\n",
      "Yield_Kg_Ha              object\n",
      "Fertilizer_Use_Kg_Ha     object\n",
      "Irrigation_Pct           object\n",
      "Notes                    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Basic CSV read - pandas will guess everything\n",
    "df_basic = pd.read_csv('crop_production_1990_2023.csv')\n",
    "\n",
    "print(f\"Shape: {df_basic.shape[0]} rows, {df_basic.shape[1]} columns\")\n",
    "print(f\"\\nColumn names:\")\n",
    "print(df_basic.columns.tolist())\n",
    "print(f\"\\nData types:\")\n",
    "print(df_basic.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Problem with Automatic Type Detection\n",
    "\n",
    "When pandas guesses types, it can make mistakes:\n",
    "- Numbers with commas become strings\n",
    "- Missing value codes like `NA` or `..` cause issues\n",
    "- Years with `.0` suffix get read as floats\n",
    "\n",
    "**Better approach:** Read everything as strings first, then convert explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All columns are now strings:\n",
      "[dtype('O')]\n",
      "\n",
      "Sample of raw data (notice the data quality issues):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>ISO3_Code</th>\n",
       "      <th>Region</th>\n",
       "      <th>Income_Group</th>\n",
       "      <th>Year</th>\n",
       "      <th>Crop</th>\n",
       "      <th>Area_Harvested_Ha</th>\n",
       "      <th>Production_Tonnes</th>\n",
       "      <th>Yield_Kg_Ha</th>\n",
       "      <th>Fertilizer_Use_Kg_Ha</th>\n",
       "      <th>Irrigation_Pct</th>\n",
       "      <th>Notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>China</td>\n",
       "      <td>CHN</td>\n",
       "      <td>East Asia</td>\n",
       "      <td>Upper middle income</td>\n",
       "      <td>2001</td>\n",
       "      <td>Soybeans</td>\n",
       "      <td>3751494</td>\n",
       "      <td>12036421.75</td>\n",
       "      <td>3208.43</td>\n",
       "      <td>100.9</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nepal</td>\n",
       "      <td>NPL</td>\n",
       "      <td>South Asia</td>\n",
       "      <td>Low income</td>\n",
       "      <td>1993</td>\n",
       "      <td>Maize</td>\n",
       "      <td>2112762</td>\n",
       "      <td>11377270.55</td>\n",
       "      <td>5385,02</td>\n",
       "      <td>19,14</td>\n",
       "      <td>9.8</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>South Korea</td>\n",
       "      <td>KOR</td>\n",
       "      <td>East Asia</td>\n",
       "      <td>High income</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>Soybeans</td>\n",
       "      <td>1650777</td>\n",
       "      <td>7474101.16</td>\n",
       "      <td>4527.63</td>\n",
       "      <td>193.84</td>\n",
       "      <td>56.6</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>United States</td>\n",
       "      <td>USA</td>\n",
       "      <td>North America</td>\n",
       "      <td>High income</td>\n",
       "      <td>2018</td>\n",
       "      <td>Wheat</td>\n",
       "      <td>4782989</td>\n",
       "      <td>32397951.41</td>\n",
       "      <td>6773.58</td>\n",
       "      <td>205.12</td>\n",
       "      <td>62.5</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Japan</td>\n",
       "      <td>JPN</td>\n",
       "      <td>East Asia</td>\n",
       "      <td>High income</td>\n",
       "      <td>2013</td>\n",
       "      <td>Rice</td>\n",
       "      <td>5434696</td>\n",
       "      <td>58322509.35</td>\n",
       "      <td>10731,51</td>\n",
       "      <td>211,64</td>\n",
       "      <td>61.4</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Country ISO3_Code         Region         Income_Group    Year  \\\n",
       "0          China       CHN      East Asia  Upper middle income    2001   \n",
       "1          Nepal       NPL     South Asia           Low income    1993   \n",
       "2    South Korea       KOR      East Asia          High income  1995.0   \n",
       "3  United States       USA  North America          High income    2018   \n",
       "4          Japan       JPN      East Asia          High income    2013   \n",
       "\n",
       "       Crop Area_Harvested_Ha Production_Tonnes Yield_Kg_Ha  \\\n",
       "0  Soybeans           3751494       12036421.75     3208.43   \n",
       "1     Maize           2112762       11377270.55     5385,02   \n",
       "2  Soybeans           1650777        7474101.16     4527.63   \n",
       "3     Wheat           4782989       32397951.41     6773.58   \n",
       "4      Rice           5434696       58322509.35    10731,51   \n",
       "\n",
       "  Fertilizer_Use_Kg_Ha Irrigation_Pct Notes  \n",
       "0                100.9                       \n",
       "1                19,14            9.8        \n",
       "2               193.84           56.6        \n",
       "3               205.12           62.5        \n",
       "4               211,64           61.4        "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Better approach: Read everything as strings\n",
    "df_raw = pd.read_csv(\n",
    "    'crop_production_1990_2023.csv',\n",
    "    dtype=str,  # Force all columns to string\n",
    "    keep_default_na=False  # Don't convert anything to NaN automatically\n",
    ")\n",
    "\n",
    "print(\"All columns are now strings:\")\n",
    "print(df_raw.dtypes.unique())\n",
    "\n",
    "print(\"\\nSample of raw data (notice the data quality issues):\")\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with European decimals: 408\n",
      "['5385,02', '10731,51', '4398,0', '2970,49', '5140,5']\n",
      "\n",
      "Missing value codes found in Production_Tonnes:\n",
      "Production_Tonnes\n",
      "NA                27\n",
      "                  26\n",
      "N/A               25\n",
      "-                 21\n",
      "**                 1\n",
      "20693827.7*        1\n",
      "11163272.65(p)     1\n",
      "6154960.74*        1\n",
      "10262733.99F       1\n",
      "3797624.37**       1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Let's find some of the data quality issues\n",
    "\n",
    "# Issue 1: European decimals (comma instead of period)\n",
    "european_decimals = df_raw[df_raw['Yield_Kg_Ha'].str.contains(',', na=False)]\n",
    "print(f\"Rows with European decimals: {len(european_decimals)}\")\n",
    "print(european_decimals['Yield_Kg_Ha'].head().tolist())\n",
    "\n",
    "# Issue 2: Various missing value codes\n",
    "print(f\"\\nMissing value codes found in Production_Tonnes:\")\n",
    "weird_values = df_raw[~df_raw['Production_Tonnes'].str.match(r'^[\\d.]+$', na=False)]['Production_Tonnes']\n",
    "print(weird_values.value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete CSV Reading Template\n",
    "\n",
    "Here is a template you can use for any CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4187 rows, 12 columns from crop_production_1990_2023.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>ISO3_Code</th>\n",
       "      <th>Region</th>\n",
       "      <th>Income_Group</th>\n",
       "      <th>Year</th>\n",
       "      <th>Crop</th>\n",
       "      <th>Area_Harvested_Ha</th>\n",
       "      <th>Production_Tonnes</th>\n",
       "      <th>Yield_Kg_Ha</th>\n",
       "      <th>Fertilizer_Use_Kg_Ha</th>\n",
       "      <th>Irrigation_Pct</th>\n",
       "      <th>Notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>China</td>\n",
       "      <td>CHN</td>\n",
       "      <td>East Asia</td>\n",
       "      <td>Upper middle income</td>\n",
       "      <td>2001</td>\n",
       "      <td>Soybeans</td>\n",
       "      <td>3751494</td>\n",
       "      <td>12036421.75</td>\n",
       "      <td>3208.43</td>\n",
       "      <td>100.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nepal</td>\n",
       "      <td>NPL</td>\n",
       "      <td>South Asia</td>\n",
       "      <td>Low income</td>\n",
       "      <td>1993</td>\n",
       "      <td>Maize</td>\n",
       "      <td>2112762</td>\n",
       "      <td>11377270.55</td>\n",
       "      <td>5385,02</td>\n",
       "      <td>19,14</td>\n",
       "      <td>9.8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>South Korea</td>\n",
       "      <td>KOR</td>\n",
       "      <td>East Asia</td>\n",
       "      <td>High income</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>Soybeans</td>\n",
       "      <td>1650777</td>\n",
       "      <td>7474101.16</td>\n",
       "      <td>4527.63</td>\n",
       "      <td>193.84</td>\n",
       "      <td>56.6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Country ISO3_Code      Region         Income_Group    Year      Crop  \\\n",
       "0        China       CHN   East Asia  Upper middle income    2001  Soybeans   \n",
       "1        Nepal       NPL  South Asia           Low income    1993     Maize   \n",
       "2  South Korea       KOR   East Asia          High income  1995.0  Soybeans   \n",
       "\n",
       "  Area_Harvested_Ha Production_Tonnes Yield_Kg_Ha Fertilizer_Use_Kg_Ha  \\\n",
       "0           3751494       12036421.75     3208.43                100.9   \n",
       "1           2112762       11377270.55     5385,02                19,14   \n",
       "2           1650777        7474101.16     4527.63               193.84   \n",
       "\n",
       "  Irrigation_Pct Notes  \n",
       "0            NaN   NaN  \n",
       "1            9.8   NaN  \n",
       "2           56.6   NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_csv_safely(filepath):\n",
    "    \"\"\"\n",
    "    Read a CSV file with full control over parsing.\n",
    "    Returns data with all columns as strings for safe processing.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\n",
    "        filepath,\n",
    "        \n",
    "        # Type control\n",
    "        dtype=str,              # Read everything as string\n",
    "        \n",
    "        # Missing value handling\n",
    "        na_values=[''],         # Only empty string is null\n",
    "        keep_default_na=False,  # Don't auto-convert NA, N/A, etc.\n",
    "        \n",
    "        # Encoding\n",
    "        encoding='utf-8',       # Most common encoding\n",
    "        \n",
    "        # Other safety options\n",
    "        low_memory=False,       # Prevent mixed type warnings\n",
    "    )\n",
    "    \n",
    "    # Add metadata\n",
    "    print(f\"Loaded {len(df)} rows, {len(df.columns)} columns from {filepath}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Use the function\n",
    "df = read_csv_safely('crop_production_1990_2023.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Working with JSON Files\n",
    "\n",
    "JSON (JavaScript Object Notation) is the standard format for API responses. You will work with JSON heavily in Assignment 2 when calling the weather API.\n",
    "\n",
    "### JSON Structure\n",
    "\n",
    "JSON has two main building blocks:\n",
    "\n",
    "1. **Objects** (like Python dictionaries): `{\"key\": \"value\"}`\n",
    "2. **Arrays** (like Python lists): `[1, 2, 3]`\n",
    "\n",
    "They can be nested inside each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample JSON file created!\n",
      "\n",
      "{\n",
      "    \"latitude\": 52.52,\n",
      "    \"longitude\": 13.41,\n",
      "    \"timezone\": \"Europe/Berlin\",\n",
      "    \"daily\": {\n",
      "        \"time\": [\"2023-01-01\", \"2023-01-02\", \"2023-01-03\"],\n",
      "        \"temperature_2m_mean\": [5.2, 3.1, 4.8],\n",
      "        \"precipitation_sum\": [0.0, 2.3, 0.5]\n",
      "    }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's create sample JSON that looks like API weather data\n",
    "# This is similar to what you'll get from Open-Meteo API in Assignment 2\n",
    "\n",
    "sample_api_response = '''\n",
    "{\n",
    "    \"latitude\": 52.52,\n",
    "    \"longitude\": 13.41,\n",
    "    \"timezone\": \"Europe/Berlin\",\n",
    "    \"daily\": {\n",
    "        \"time\": [\"2023-01-01\", \"2023-01-02\", \"2023-01-03\"],\n",
    "        \"temperature_2m_mean\": [5.2, 3.1, 4.8],\n",
    "        \"precipitation_sum\": [0.0, 2.3, 0.5]\n",
    "    }\n",
    "}\n",
    "'''\n",
    "\n",
    "# Save it to a file\n",
    "with open('sample_weather.json', 'w') as f:\n",
    "    f.write(sample_api_response)\n",
    "\n",
    "print(\"Sample JSON file created!\")\n",
    "print(sample_api_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'dict'>\n",
      "Top-level keys: ['latitude', 'longitude', 'timezone', 'daily']\n"
     ]
    }
   ],
   "source": [
    "# Reading JSON with Python's json module\n",
    "\n",
    "with open('sample_weather.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Now 'data' is a Python dictionary\n",
    "print(f\"Type: {type(data)}\")\n",
    "print(f\"Top-level keys: {list(data.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latitude: 52.52\n",
      "\n",
      "Daily data keys: ['time', 'temperature_2m_mean', 'precipitation_sum']\n",
      "\n",
      "Temperatures: [5.2, 3.1, 4.8]\n"
     ]
    }
   ],
   "source": [
    "# Accessing nested data\n",
    "\n",
    "# Get latitude\n",
    "lat = data['latitude']\n",
    "print(f\"Latitude: {lat}\")\n",
    "\n",
    "# Get the daily data (this is nested)\n",
    "daily = data['daily']\n",
    "print(f\"\\nDaily data keys: {list(daily.keys())}\")\n",
    "\n",
    "# Get temperatures\n",
    "temps = daily['temperature_2m_mean']\n",
    "print(f\"\\nTemperatures: {temps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather data as DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>temp_mean</th>\n",
       "      <th>precip_sum</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.52</td>\n",
       "      <td>13.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>52.52</td>\n",
       "      <td>13.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>52.52</td>\n",
       "      <td>13.41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  temp_mean  precip_sum  latitude  longitude\n",
       "0  2023-01-01        5.2         0.0     52.52      13.41\n",
       "1  2023-01-02        3.1         2.3     52.52      13.41\n",
       "2  2023-01-03        4.8         0.5     52.52      13.41"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting nested JSON to a DataFrame\n",
    "# This is what you'll do with API responses\n",
    "\n",
    "# Method: Manual construction (gives you full control)\n",
    "df_weather = pd.DataFrame({\n",
    "    'date': data['daily']['time'],\n",
    "    'temp_mean': data['daily']['temperature_2m_mean'],\n",
    "    'precip_sum': data['daily']['precipitation_sum']\n",
    "})\n",
    "\n",
    "# Add metadata from the parent object\n",
    "df_weather['latitude'] = data['latitude']\n",
    "df_weather['longitude'] = data['longitude']\n",
    "\n",
    "print(\"Weather data as DataFrame:\")\n",
    "df_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened structure:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>weather</th>\n",
       "      <th>coordinates.lat</th>\n",
       "      <th>coordinates.lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Canada</td>\n",
       "      <td>[{'date': '2023-01-01', 'temp': -15.2}, {'date...</td>\n",
       "      <td>56.13</td>\n",
       "      <td>-106.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>[{'date': '2023-01-01', 'temp': 28.3}, {'date'...</td>\n",
       "      <td>-14.24</td>\n",
       "      <td>-51.93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country                                            weather  coordinates.lat  \\\n",
       "0  Canada  [{'date': '2023-01-01', 'temp': -15.2}, {'date...            56.13   \n",
       "1  Brazil  [{'date': '2023-01-01', 'temp': 28.3}, {'date'...           -14.24   \n",
       "\n",
       "   coordinates.lon  \n",
       "0          -106.35  \n",
       "1           -51.93  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using json_normalize for complex nested structures\n",
    "from pandas import json_normalize\n",
    "\n",
    "# Create more complex sample data\n",
    "complex_data = [\n",
    "    {\n",
    "        \"country\": \"Canada\",\n",
    "        \"coordinates\": {\"lat\": 56.13, \"lon\": -106.35},\n",
    "        \"weather\": [\n",
    "            {\"date\": \"2023-01-01\", \"temp\": -15.2},\n",
    "            {\"date\": \"2023-01-02\", \"temp\": -18.5}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Brazil\",\n",
    "        \"coordinates\": {\"lat\": -14.24, \"lon\": -51.93},\n",
    "        \"weather\": [\n",
    "            {\"date\": \"2023-01-01\", \"temp\": 28.3},\n",
    "            {\"date\": \"2023-01-02\", \"temp\": 29.1}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Flatten the nested coordinates\n",
    "df_flat = json_normalize(complex_data)\n",
    "print(\"Flattened structure:\")\n",
    "df_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding nested arrays\n",
    "df_expanded = json_normalize(\n",
    "    complex_data,\n",
    "    record_path='weather',      # The array to expand\n",
    "    meta=['country',            # Keep these fields\n",
    "          ['coordinates', 'lat'],\n",
    "          ['coordinates', 'lon']]\n",
    ")\n",
    "\n",
    "print(\"Expanded weather data (one row per weather observation):\")\n",
    "df_expanded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Raw Layer vs Cleaned Layer\n",
    "\n",
    "This is one of the most important concepts in data engineering.\n",
    "\n",
    "### The Problem with Direct Transformation\n",
    "\n",
    "If you transform data directly:\n",
    "- You lose the original data\n",
    "- If something goes wrong, you cannot debug\n",
    "- If cleaning logic changes, you must re-extract from source\n",
    "\n",
    "### The Solution: Layered Architecture\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                     SOURCE FILES / APIs                     │\n",
    "└─────────────────────────┬───────────────────────────────────┘\n",
    "                          │ Extract (no changes)\n",
    "                          ▼\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                      RAW LAYER                              │\n",
    "│   - Exact copy of source                                    │\n",
    "│   - All data as strings                                     │\n",
    "│   - Add: source file, extraction timestamp, row number      │\n",
    "│   - Never modify this layer                                 │\n",
    "└─────────────────────────┬───────────────────────────────────┘\n",
    "                          │ Transform\n",
    "                          ▼\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                    CLEANED LAYER                            │\n",
    "│   - Correct data types                                      │\n",
    "│   - Standardized formats                                    │\n",
    "│   - Nulls handled consistently                              │\n",
    "│   - Validated                                               │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory structure created:\n",
      "  data/\n",
      "    raw/      <- Untouched source data\n",
      "    cleaned/  <- Transformed data\n"
     ]
    }
   ],
   "source": [
    "# Create directory structure for our layers\n",
    "import os\n",
    "\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "os.makedirs('data/cleaned', exist_ok=True)\n",
    "\n",
    "print(\"Directory structure created:\")\n",
    "print(\"  data/\")\n",
    "print(\"    raw/      <- Untouched source data\")\n",
    "print(\"    cleaned/  <- Transformed data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 4187 rows from crop_production_1990_2023.csv\n",
      "\n",
      "Metadata columns:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Year</th>\n",
       "      <th>_source_file</th>\n",
       "      <th>_extracted_at</th>\n",
       "      <th>_row_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>China</td>\n",
       "      <td>2001</td>\n",
       "      <td>crop_production_1990_2023.csv</td>\n",
       "      <td>2026-01-14T09:30:13.131857</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nepal</td>\n",
       "      <td>1993</td>\n",
       "      <td>crop_production_1990_2023.csv</td>\n",
       "      <td>2026-01-14T09:30:13.131857</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>South Korea</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>crop_production_1990_2023.csv</td>\n",
       "      <td>2026-01-14T09:30:13.131857</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Country    Year                   _source_file  \\\n",
       "0        China    2001  crop_production_1990_2023.csv   \n",
       "1        Nepal    1993  crop_production_1990_2023.csv   \n",
       "2  South Korea  1995.0  crop_production_1990_2023.csv   \n",
       "\n",
       "                _extracted_at  _row_num  \n",
       "0  2026-01-14T09:30:13.131857         1  \n",
       "1  2026-01-14T09:30:13.131857         2  \n",
       "2  2026-01-14T09:30:13.131857         3  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Extract to Raw Layer\n",
    "\n",
    "def extract_to_raw(source_file, source_name):\n",
    "    \"\"\"\n",
    "    Extract data from source file to raw layer.\n",
    "    Adds metadata columns but makes NO transformations.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    source_file : str\n",
    "        Path to the source file\n",
    "    source_name : str\n",
    "        Name to identify this data source\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with raw data plus metadata columns\n",
    "    \"\"\"\n",
    "    # Read as strings - no type conversion\n",
    "    df = pd.read_csv(source_file, dtype=str, keep_default_na=False)\n",
    "    \n",
    "    # Add metadata columns (prefix with _ to mark as system columns)\n",
    "    df['_source_file'] = os.path.basename(source_file)\n",
    "    df['_source_name'] = source_name\n",
    "    df['_extracted_at'] = datetime.now().isoformat()\n",
    "    df['_row_num'] = range(1, len(df) + 1)\n",
    "    \n",
    "    print(f\"Extracted {len(df)} rows from {source_file}\")\n",
    "    return df\n",
    "\n",
    "# Extract crop production data\n",
    "df_raw = extract_to_raw('crop_production_1990_2023.csv', 'fao_crop_production')\n",
    "\n",
    "# Show metadata columns\n",
    "print(\"\\nMetadata columns:\")\n",
    "df_raw[['Country', 'Year', '_source_file', '_extracted_at', '_row_num']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: data/raw/crop_production_raw_20260114_093017.csv\n"
     ]
    }
   ],
   "source": [
    "# Save raw layer with timestamp in filename\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "raw_file = f'data/raw/crop_production_raw_{timestamp}.csv'\n",
    "df_raw.to_csv(raw_file, index=False)\n",
    "print(f\"Saved to: {raw_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing clean_numeric:\n",
      "  '1234.56' -> 1234.56\n",
      "  '1234,56' -> 1234.56\n",
      "  'NA' -> None\n",
      "  '1234*' -> 1234.0\n",
      "  '1234(e)' -> 1234.0\n",
      "  '' -> None\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Transform to Cleaned Layer\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_numeric(value):\n",
    "    \"\"\"Convert string to float, handling various formats.\"\"\"\n",
    "    if pd.isna(value) or value == '':\n",
    "        return None\n",
    "    \n",
    "    value = str(value).strip()\n",
    "    \n",
    "    # Check for missing value codes\n",
    "    if value in ['NA', 'N/A', '..', '-', 'NULL']:\n",
    "        return None\n",
    "    \n",
    "    # Remove footnote markers like *, **, (e), (p)\n",
    "    value = re.sub(r'[*]+$', '', value)\n",
    "    value = re.sub(r'\\([a-zA-Z]\\)$', '', value)\n",
    "    \n",
    "    # Handle European decimals (comma as decimal separator)\n",
    "    if ',' in value and '.' not in value:\n",
    "        value = value.replace(',', '.')\n",
    "    \n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def clean_year(value):\n",
    "    \"\"\"Convert year string to integer.\"\"\"\n",
    "    if pd.isna(value) or value == '':\n",
    "        return None\n",
    "    try:\n",
    "        # Handle years like \"2020.0\"\n",
    "        return int(float(str(value).strip()))\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Test the functions\n",
    "print(\"Testing clean_numeric:\")\n",
    "test_values = ['1234.56', '1234,56', 'NA', '1234*', '1234(e)', '']\n",
    "for v in test_values:\n",
    "    print(f\"  '{v}' -> {clean_numeric(v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types after cleaning:\n",
      "Year                   int64\n",
      "Yield_Kg_Ha          float64\n",
      "Production_Tonnes    float64\n",
      "dtype: object\n",
      "\n",
      "Sample cleaned data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Year</th>\n",
       "      <th>Crop</th>\n",
       "      <th>Yield_Kg_Ha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>China</td>\n",
       "      <td>2001</td>\n",
       "      <td>Soybeans</td>\n",
       "      <td>3208.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nepal</td>\n",
       "      <td>1993</td>\n",
       "      <td>Maize</td>\n",
       "      <td>5385.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>South Korea</td>\n",
       "      <td>1995</td>\n",
       "      <td>Soybeans</td>\n",
       "      <td>4527.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>United States</td>\n",
       "      <td>2018</td>\n",
       "      <td>Wheat</td>\n",
       "      <td>6773.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Japan</td>\n",
       "      <td>2013</td>\n",
       "      <td>Rice</td>\n",
       "      <td>10731.51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Country  Year      Crop  Yield_Kg_Ha\n",
       "0          China  2001  Soybeans      3208.43\n",
       "1          Nepal  1993     Maize      5385.02\n",
       "2    South Korea  1995  Soybeans      4527.63\n",
       "3  United States  2018     Wheat      6773.58\n",
       "4          Japan  2013      Rice     10731.51"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_to_cleaned(df_raw):\n",
    "    \"\"\"\n",
    "    Transform raw data to cleaned layer.\n",
    "    Applies type conversions and standardization.\n",
    "    \"\"\"\n",
    "    df = df_raw.copy()\n",
    "    \n",
    "    # Clean text columns (strip whitespace)\n",
    "    text_columns = ['Country', 'ISO3_Code', 'Region', 'Income_Group', 'Crop', 'Notes']\n",
    "    for col in text_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].str.strip()\n",
    "    \n",
    "    # Clean Year column\n",
    "    df['Year'] = df['Year'].apply(clean_year)\n",
    "    \n",
    "    # Clean numeric columns\n",
    "    numeric_columns = ['Area_Harvested_Ha', 'Production_Tonnes', 'Yield_Kg_Ha',\n",
    "                       'Fertilizer_Use_Kg_Ha', 'Irrigation_Pct']\n",
    "    for col in numeric_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(clean_numeric)\n",
    "    \n",
    "    # Add cleaning metadata\n",
    "    df['_cleaned_at'] = datetime.now().isoformat()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Transform the data\n",
    "df_cleaned = transform_to_cleaned(df_raw)\n",
    "\n",
    "print(\"Data types after cleaning:\")\n",
    "print(df_cleaned[['Year', 'Yield_Kg_Ha', 'Production_Tonnes']].dtypes)\n",
    "\n",
    "print(\"\\nSample cleaned data:\")\n",
    "df_cleaned[['Country', 'Year', 'Crop', 'Yield_Kg_Ha']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: data/cleaned/crop_production_cleaned_20260114_093017.csv\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned layer\n",
    "cleaned_file = f'data/cleaned/crop_production_cleaned_{timestamp}.csv'\n",
    "df_cleaned.to_csv(cleaned_file, index=False)\n",
    "print(f\"Saved to: {cleaned_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Data Lineage: Where Did This Data Come From?\n",
    "\n",
    "**Data lineage** tracks the origin and transformations of data. This is essential for:\n",
    "\n",
    "- **Debugging** - Finding where problems came from\n",
    "- **Auditing** - Proving data sources for compliance\n",
    "- **Reproducibility** - Recreating analysis\n",
    "\n",
    "For this course, we use a simple approach:\n",
    "\n",
    "1. Metadata columns in the data (`_source_file`, `_extracted_at`)\n",
    "2. A README file documenting the pipeline\n",
    "3. Log files recording what happened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Data Lineage Documentation\n",
      "\n",
      "Generated: 2026-01-14T09:30:29.381548\n",
      "\n",
      "## Source\n",
      "- File: crop_production_1990_2023.csv\n",
      "\n",
      "## Raw Layer\n",
      "- File: data/raw/crop_production_raw_20260114_093017.csv\n",
      "- Contains exact copy of source with metadata columns added\n",
      "\n",
      "## Cleaned Layer  \n",
      "- File: data/cleaned/crop_production_cleaned_20260114_093017.csv\n",
      "\n",
      "## Transformations Applied\n",
      "- Stripped whitespace from text columns\n",
      "- Converted Year from string to integer (handled X.0 format)\n",
      "- Converted numeric columns to float (handled European decimals)\n",
      "- Replaced missing codes (NA, .., -, N/A) with NULL\n",
      "- Removed footnote markers (*, **, (e), (p)) from numbers\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a lineage README\n",
    "\n",
    "def create_lineage_readme(source_file, raw_file, cleaned_file, transformations):\n",
    "    \"\"\"\n",
    "    Create a README documenting the data pipeline.\n",
    "    \"\"\"\n",
    "    readme = f\"\"\"\n",
    "# Data Lineage Documentation\n",
    "\n",
    "Generated: {datetime.now().isoformat()}\n",
    "\n",
    "## Source\n",
    "- File: {source_file}\n",
    "\n",
    "## Raw Layer\n",
    "- File: {raw_file}\n",
    "- Contains exact copy of source with metadata columns added\n",
    "\n",
    "## Cleaned Layer  \n",
    "- File: {cleaned_file}\n",
    "\n",
    "## Transformations Applied\n",
    "\"\"\"\n",
    "    for t in transformations:\n",
    "        readme += f\"- {t}\\n\"\n",
    "    \n",
    "    return readme\n",
    "\n",
    "# Document what we did\n",
    "transformations = [\n",
    "    \"Stripped whitespace from text columns\",\n",
    "    \"Converted Year from string to integer (handled X.0 format)\",\n",
    "    \"Converted numeric columns to float (handled European decimals)\",\n",
    "    \"Replaced missing codes (NA, .., -, N/A) with NULL\",\n",
    "    \"Removed footnote markers (*, **, (e), (p)) from numbers\"\n",
    "]\n",
    "\n",
    "readme = create_lineage_readme(\n",
    "    'crop_production_1990_2023.csv',\n",
    "    raw_file,\n",
    "    cleaned_file,\n",
    "    transformations\n",
    ")\n",
    "\n",
    "# Save README\n",
    "with open('data/cleaned/README.md', 'w') as f:\n",
    "    f.write(readme)\n",
    "\n",
    "print(readme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Complete ETL Pipeline Example\n",
    "\n",
    "Let's put everything together into a reusable pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ETL Pipeline: crop_production\n",
      "============================================================\n",
      "\n",
      "[1/3] EXTRACT...\n",
      "Extracted 4187 rows from crop_production_1990_2023.csv\n",
      "\n",
      "[2/3] TRANSFORM...\n",
      "\n",
      "[3/3] LOAD...\n",
      "\n",
      "============================================================\n",
      "Pipeline complete!\n",
      "  Rows extracted: 4187\n",
      "  Rows cleaned: 4187\n",
      "  Output: data/cleaned/crop_production_cleaned_20260114_093034.csv\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def run_etl_pipeline(source_file, source_name):\n",
    "    \"\"\"\n",
    "    Complete ETL pipeline for CSV files.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    source_file : str\n",
    "        Path to source CSV file\n",
    "    source_name : str\n",
    "        Identifier for this data source\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with pipeline results\n",
    "    \"\"\"\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"ETL Pipeline: {source_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    results = {'source': source_file, 'timestamp': timestamp}\n",
    "    \n",
    "    # EXTRACT\n",
    "    print(\"\\n[1/3] EXTRACT...\")\n",
    "    df_raw = extract_to_raw(source_file, source_name)\n",
    "    results['rows_extracted'] = len(df_raw)\n",
    "    \n",
    "    # Save raw\n",
    "    raw_path = f'data/raw/{source_name}_raw_{timestamp}.csv'\n",
    "    df_raw.to_csv(raw_path, index=False)\n",
    "    results['raw_file'] = raw_path\n",
    "    \n",
    "    # TRANSFORM\n",
    "    print(\"\\n[2/3] TRANSFORM...\")\n",
    "    df_cleaned = transform_to_cleaned(df_raw)\n",
    "    results['rows_cleaned'] = len(df_cleaned)\n",
    "    \n",
    "    # LOAD (save cleaned)\n",
    "    print(\"\\n[3/3] LOAD...\")\n",
    "    cleaned_path = f'data/cleaned/{source_name}_cleaned_{timestamp}.csv'\n",
    "    df_cleaned.to_csv(cleaned_path, index=False)\n",
    "    results['cleaned_file'] = cleaned_path\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Pipeline complete!\")\n",
    "    print(f\"  Rows extracted: {results['rows_extracted']}\")\n",
    "    print(f\"  Rows cleaned: {results['rows_cleaned']}\")\n",
    "    print(f\"  Output: {cleaned_path}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return results, df_cleaned\n",
    "\n",
    "# Run the pipeline\n",
    "results, df_final = run_etl_pipeline(\n",
    "    'crop_production_1990_2023.csv',\n",
    "    'crop_production'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **ETL = Extract, Transform, Load**\n",
    "   - Automated pipelines for recurring data\n",
    "\n",
    "2. **CSV Reading**\n",
    "   - Read as strings first for safety\n",
    "   - Handle encodings, delimiters, missing values explicitly\n",
    "\n",
    "3. **JSON Parsing**\n",
    "   - Use `json.load()` for files\n",
    "   - Use `json_normalize()` for nested structures\n",
    "\n",
    "4. **Layered Architecture**\n",
    "   - Raw layer: exact copy with metadata\n",
    "   - Cleaned layer: transformed and validated\n",
    "\n",
    "5. **Data Lineage**\n",
    "   - Track source, timestamp, transformations\n",
    "   - Document in README files\n",
    "\n",
    "---\n",
    "\n",
    "### Next Class: ETL Pipeline II - APIs and Parameters\n",
    "\n",
    "On Wednesday, we will learn:\n",
    "- How to call REST APIs from Python\n",
    "- The Open-Meteo weather API (used in Assignment 2)\n",
    "- Handling authentication and rate limits\n",
    "- Pagination for large datasets\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
