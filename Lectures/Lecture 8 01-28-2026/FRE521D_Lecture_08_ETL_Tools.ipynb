{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FRE 521D: Data Analytics in Climate, Food and Environment\n",
    "## Lecture 8: ETL Tools - BigQuery and Python Automation\n",
    "\n",
    "**Date:** Wednesday, January 28, 2026  \n",
    "**Instructor:** Asif Ahmed Neloy  \n",
    "**Program:** UBC Master of Food and Resource Economics\n",
    "\n",
    "---\n",
    "\n",
    "### Today's Agenda\n",
    "\n",
    "1. Introduction to Cloud Data Warehouses\n",
    "2. Google BigQuery Fundamentals\n",
    "3. Connecting to BigQuery from Python\n",
    "4. Loading Large Datasets Efficiently\n",
    "5. Python Automation Scripts for ETL\n",
    "6. Scheduling and Task Automation\n",
    "7. Error Handling and Logging in Production\n",
    "8. Monitoring Pipeline Health\n",
    "9. Complete Automated ETL Example\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this lecture, you will be able to:\n",
    "\n",
    "1. Explain the difference between traditional databases and cloud data warehouses\n",
    "2. Connect to Google BigQuery from Python and execute queries\n",
    "3. Load large datasets efficiently using batch operations\n",
    "4. Write automated ETL scripts with proper logging\n",
    "5. Implement error handling with retry logic\n",
    "6. Design monitoring strategies for production pipelines\n",
    "7. Schedule ETL jobs to run automatically\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 2.3.3\n",
      "Current time: 2026-01-27 21:54:13\n",
      "Ready for ETL Tools lecture!\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_rows', 60)\n",
    "\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"Ready for ETL Tools lecture!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell above imports the essential libraries we need for this lecture. We use `pandas` for data manipulation, `logging` for tracking pipeline execution, and `datetime` for timestamp operations. The display settings ensure we can see all columns in our DataFrames without truncation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Cloud Data Warehouses\n",
    "\n",
    "### What is a Data Warehouse?\n",
    "\n",
    "A **data warehouse** is a centralized repository designed for analytical queries on large volumes of data. Unlike transactional databases (OLTP) that handle many small read/write operations, data warehouses (OLAP) are optimized for complex queries that scan millions of rows.\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                    DATA WAREHOUSE vs DATABASE                   │\n",
    "├─────────────────────────────────┬───────────────────────────────┤\n",
    "│      TRANSACTIONAL (OLTP)       │      ANALYTICAL (OLAP)        │\n",
    "├─────────────────────────────────┼───────────────────────────────┤\n",
    "│  MySQL, PostgreSQL, SQL Server  │  BigQuery, Snowflake, Redshift│\n",
    "│  Many small read/write ops      │  Few large read-heavy queries │\n",
    "│  Row-oriented storage           │  Column-oriented storage      │\n",
    "│  Normalized schemas             │  Denormalized/star schemas    │\n",
    "│  Real-time operations           │  Batch processing             │\n",
    "│  Gigabytes of data              │  Terabytes to Petabytes       │\n",
    "└─────────────────────────────────┴───────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Why Cloud Data Warehouses?\n",
    "\n",
    "Traditional on-premise data warehouses require:\n",
    "- Expensive hardware\n",
    "- Dedicated IT staff\n",
    "- Capacity planning months in advance\n",
    "- Manual scaling\n",
    "\n",
    "Cloud data warehouses provide:\n",
    "- **Pay-per-query pricing**: Only pay for what you use\n",
    "- **Automatic scaling**: Handle any data volume\n",
    "- **No infrastructure management**: Focus on analysis, not servers\n",
    "- **Built-in redundancy**: Data is replicated automatically\n",
    "\n",
    "### Major Cloud Data Warehouses\n",
    "\n",
    "| Platform | Provider | Key Feature |\n",
    "|----------|----------|-------------|\n",
    "| BigQuery | Google | Serverless, pay-per-query |\n",
    "| Snowflake | Independent | Multi-cloud, data sharing |\n",
    "| Redshift | Amazon | Tight AWS integration |\n",
    "| Azure Synapse | Microsoft | Unified analytics platform |\n",
    "\n",
    "In this course, we focus on **Google BigQuery** because:\n",
    "1. Generous free tier (1 TB queries/month, 10 GB storage)\n",
    "2. No cluster management required\n",
    "3. Excellent Python integration\n",
    "4. Widely used in industry\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Google BigQuery Fundamentals\n",
    "\n",
    "### BigQuery Architecture\n",
    "\n",
    "BigQuery separates storage and compute, which is the key to its scalability:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                      BIGQUERY ARCHITECTURE                      │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│                                                                 │\n",
    "│   ┌─────────────┐      ┌─────────────┐      ┌─────────────┐   │\n",
    "│   │   Client    │ ───► │   Dremel    │ ───► │   Colossus  │   │\n",
    "│   │  (Python)   │      │  (Compute)  │      │  (Storage)  │   │\n",
    "│   └─────────────┘      └─────────────┘      └─────────────┘   │\n",
    "│                                                                 │\n",
    "│   You send SQL         Thousands of         Distributed        │\n",
    "│   queries here         workers process      column-store       │\n",
    "│                        your query           file system        │\n",
    "│                                                                 │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### BigQuery Hierarchy\n",
    "\n",
    "```\n",
    "Project (billing unit)\n",
    "    └── Dataset (like a schema/database)\n",
    "            └── Table (your data)\n",
    "                    └── Columns and Rows\n",
    "```\n",
    "\n",
    "- **Project**: Top-level container, linked to billing account\n",
    "- **Dataset**: Collection of tables, controls access permissions\n",
    "- **Table**: Actual data, can be native or external\n",
    "\n",
    "### BigQuery Pricing\n",
    "\n",
    "Two pricing models:\n",
    "\n",
    "1. **On-demand** (pay per query)\n",
    "   - $5 per TB of data scanned\n",
    "   - First 1 TB/month free\n",
    "   - Good for ad-hoc analysis\n",
    "\n",
    "2. **Flat-rate** (reserved capacity)\n",
    "   - Fixed monthly cost for dedicated slots\n",
    "   - Better for predictable, heavy workloads\n",
    "\n",
    "**Storage costs:**\n",
    "- Active storage: $0.02/GB/month\n",
    "- Long-term storage (>90 days untouched): $0.01/GB/month\n",
    "- First 10 GB/month free\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Connecting to BigQuery from Python\n",
    "\n",
    "### Installation\n",
    "\n",
    "First, install the required libraries:\n",
    "\n",
    "```bash\n",
    "pip install google-cloud-bigquery pandas-gbq db-dtypes\n",
    "```\n",
    "\n",
    "### Authentication Options\n",
    "\n",
    "BigQuery requires authentication. There are several methods:\n",
    "\n",
    "1. **Service Account Key** (recommended for automation)\n",
    "   - Download JSON key from Google Cloud Console\n",
    "   - Set environment variable: `GOOGLE_APPLICATION_CREDENTIALS`\n",
    "\n",
    "2. **User Account** (for interactive use)\n",
    "   - Run `gcloud auth application-default login`\n",
    "   - Opens browser for OAuth\n",
    "\n",
    "3. **Default Credentials** (on Google Cloud VMs)\n",
    "   - Automatic if running on GCP\n",
    "\n",
    "For this lecture, we will simulate BigQuery operations using SQLite to demonstrate the concepts without requiring cloud credentials. The patterns are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to simulated BigQuery (SQLite: weather_warehouse.db)\n"
     ]
    }
   ],
   "source": [
    "# Since we may not have BigQuery credentials, let's create a simulation\n",
    "# that demonstrates the same patterns using SQLite\n",
    "\n",
    "import sqlite3\n",
    "from contextlib import contextmanager\n",
    "\n",
    "class BigQuerySimulator:\n",
    "    \"\"\"\n",
    "    A class that simulates BigQuery operations using SQLite.\n",
    "    This allows us to learn BigQuery patterns without cloud credentials.\n",
    "    The methods mirror the actual google-cloud-bigquery API.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, database_path=':memory:'):\n",
    "        \"\"\"\n",
    "        Initialize the simulator with a SQLite database.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        database_path : str\n",
    "            Path to SQLite database file, or ':memory:' for in-memory\n",
    "        \"\"\"\n",
    "        self.database_path = database_path\n",
    "        self.connection = sqlite3.connect(database_path)\n",
    "        print(f\"Connected to simulated BigQuery (SQLite: {database_path})\")\n",
    "    \n",
    "    def query(self, sql):\n",
    "        \"\"\"\n",
    "        Execute a SQL query and return results as DataFrame.\n",
    "        Mirrors: client.query(sql).to_dataframe()\n",
    "        \"\"\"\n",
    "        return pd.read_sql_query(sql, self.connection)\n",
    "    \n",
    "    def load_table_from_dataframe(self, dataframe, table_name, if_exists='replace'):\n",
    "        \"\"\"\n",
    "        Load a DataFrame into a table.\n",
    "        Mirrors: client.load_table_from_dataframe(df, table_ref)\n",
    "        \"\"\"\n",
    "        dataframe.to_sql(table_name, self.connection, if_exists=if_exists, index=False)\n",
    "        row_count = len(dataframe)\n",
    "        print(f\"Loaded {row_count:,} rows into {table_name}\")\n",
    "        return row_count\n",
    "    \n",
    "    def list_tables(self):\n",
    "        \"\"\"\n",
    "        List all tables in the database.\n",
    "        Mirrors: client.list_tables(dataset)\n",
    "        \"\"\"\n",
    "        query = \"SELECT name FROM sqlite_master WHERE type='table'\"\n",
    "        return pd.read_sql_query(query, self.connection)['name'].tolist()\n",
    "    \n",
    "    def get_table_info(self, table_name):\n",
    "        \"\"\"\n",
    "        Get schema information for a table.\n",
    "        Mirrors: client.get_table(table_ref)\n",
    "        \"\"\"\n",
    "        query = f\"PRAGMA table_info({table_name})\"\n",
    "        return pd.read_sql_query(query, self.connection)\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close the database connection.\"\"\"\n",
    "        self.connection.close()\n",
    "        print(\"Connection closed.\")\n",
    "\n",
    "\n",
    "# Create our simulated BigQuery client\n",
    "bq_client = BigQuerySimulator('weather_warehouse.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `BigQuerySimulator` class above creates a local simulation of BigQuery using SQLite. This is valuable for learning because:\n",
    "\n",
    "1. **No cloud credentials required** - You can run this code immediately\n",
    "2. **Same patterns** - The method names and usage mirror the real BigQuery API\n",
    "3. **Fast iteration** - No network latency during development\n",
    "\n",
    "In production, you would replace this with the actual BigQuery client:\n",
    "\n",
    "```python\n",
    "from google.cloud import bigquery\n",
    "bq_client = bigquery.Client(project='your-project-id')\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loading Large Datasets Efficiently\n",
    "\n",
    "### The Challenge with Large Data\n",
    "\n",
    "Our `GlobalWeatherRepository.csv` has nearly 44,000 rows. When loading large datasets, we face several challenges:\n",
    "\n",
    "1. **Memory constraints**: Loading everything at once may exceed RAM\n",
    "2. **Network timeouts**: Large uploads can fail partway through\n",
    "3. **Type inference**: Pandas may guess wrong types on large files\n",
    "4. **Progress tracking**: Need to know how much is done\n",
    "\n",
    "### Strategy: Chunked Loading\n",
    "\n",
    "The solution is to load data in **chunks** - smaller pieces that are manageable:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                     CHUNKED LOADING STRATEGY                    │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│                                                                 │\n",
    "│   Large CSV File                    Database Table              │\n",
    "│   ┌──────────────┐                  ┌──────────────┐           │\n",
    "│   │  Chunk 1     │ ───────────────► │  Rows 1-10K  │           │\n",
    "│   │  (10K rows)  │                  ├──────────────┤           │\n",
    "│   ├──────────────┤                  │  Rows 10K-20K│           │\n",
    "│   │  Chunk 2     │ ───────────────► ├──────────────┤           │\n",
    "│   │  (10K rows)  │                  │  Rows 20K-30K│           │\n",
    "│   ├──────────────┤                  ├──────────────┤           │\n",
    "│   │  Chunk 3     │ ───────────────► │  Rows 30K-44K│           │\n",
    "│   │  (10K rows)  │                  └──────────────┘           │\n",
    "│   ├──────────────┤                                             │\n",
    "│   │  Chunk 4     │                                             │\n",
    "│   │  (4K rows)   │                                             │\n",
    "│   └──────────────┘                                             │\n",
    "│                                                                 │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 10.95 MB\n",
      "Total rows: 43,884\n",
      "\n",
      "Columns (41):\n",
      "   1. country\n",
      "   2. location_name\n",
      "   3. latitude\n",
      "   4. longitude\n",
      "   5. timezone\n",
      "   6. last_updated_epoch\n",
      "   7. last_updated\n",
      "   8. temperature_celsius\n",
      "   9. temperature_fahrenheit\n",
      "  10. condition_text\n",
      "  11. wind_mph\n",
      "  12. wind_kph\n",
      "  13. wind_degree\n",
      "  14. wind_direction\n",
      "  15. pressure_mb\n",
      "  16. pressure_in\n",
      "  17. precip_mm\n",
      "  18. precip_in\n",
      "  19. humidity\n",
      "  20. cloud\n",
      "  21. feels_like_celsius\n",
      "  22. feels_like_fahrenheit\n",
      "  23. visibility_km\n",
      "  24. visibility_miles\n",
      "  25. uv_index\n",
      "  26. gust_mph\n",
      "  27. gust_kph\n",
      "  28. air_quality_Carbon_Monoxide\n",
      "  29. air_quality_Ozone\n",
      "  30. air_quality_Nitrogen_dioxide\n",
      "  31. air_quality_Sulphur_dioxide\n",
      "  32. air_quality_PM2.5\n",
      "  33. air_quality_PM10\n",
      "  34. air_quality_us-epa-index\n",
      "  35. air_quality_gb-defra-index\n",
      "  36. sunrise\n",
      "  37. sunset\n",
      "  38. moonrise\n",
      "  39. moonset\n",
      "  40. moon_phase\n",
      "  41. moon_illumination\n"
     ]
    }
   ],
   "source": [
    "# First, let's examine our dataset\n",
    "DATA_PATH = '../../Datasets/GlobalWeatherRepository.csv'\n",
    "\n",
    "# Check file size\n",
    "file_size = os.path.getsize(DATA_PATH)\n",
    "print(f\"File size: {file_size / (1024*1024):.2f} MB\")\n",
    "\n",
    "# Count rows without loading entire file\n",
    "with open(DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "    row_count = sum(1 for line in f) - 1  # subtract header\n",
    "print(f\"Total rows: {row_count:,}\")\n",
    "\n",
    "# Preview the structure\n",
    "df_preview = pd.read_csv(DATA_PATH, nrows=5)\n",
    "print(f\"\\nColumns ({len(df_preview.columns)}):\")\n",
    "for i, col in enumerate(df_preview.columns):\n",
    "    print(f\"  {i+1:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above examines our dataset without loading it entirely into memory. We learn:\n",
    "- The file size in megabytes\n",
    "- The exact row count\n",
    "- All column names\n",
    "\n",
    "This reconnaissance step is essential before designing your ETL pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema defined with 41 columns\n",
      "\n",
      "Data types distribution:\n",
      "  float64: 29 columns\n",
      "  string: 11 columns\n",
      "  int64: 1 columns\n"
     ]
    }
   ],
   "source": [
    "# Define the schema explicitly for consistent data types\n",
    "# This prevents pandas from guessing and ensures consistency across chunks\n",
    "\n",
    "WEATHER_SCHEMA = {\n",
    "    'country': 'string',\n",
    "    'location_name': 'string',\n",
    "    'latitude': 'float64',\n",
    "    'longitude': 'float64',\n",
    "    'timezone': 'string',\n",
    "    'last_updated_epoch': 'int64',\n",
    "    'last_updated': 'string',\n",
    "    'temperature_celsius': 'float64',\n",
    "    'temperature_fahrenheit': 'float64',\n",
    "    'condition_text': 'string',\n",
    "    'wind_mph': 'float64',\n",
    "    'wind_kph': 'float64',\n",
    "    'wind_degree': 'float64',\n",
    "    'wind_direction': 'string',\n",
    "    'pressure_mb': 'float64',\n",
    "    'pressure_in': 'float64',\n",
    "    'precip_mm': 'float64',\n",
    "    'precip_in': 'float64',\n",
    "    'humidity': 'float64',\n",
    "    'cloud': 'float64',\n",
    "    'feels_like_celsius': 'float64',\n",
    "    'feels_like_fahrenheit': 'float64',\n",
    "    'visibility_km': 'float64',\n",
    "    'visibility_miles': 'float64',\n",
    "    'uv_index': 'float64',\n",
    "    'gust_mph': 'float64',\n",
    "    'gust_kph': 'float64',\n",
    "    'air_quality_Carbon_Monoxide': 'float64',\n",
    "    'air_quality_Ozone': 'float64',\n",
    "    'air_quality_Nitrogen_dioxide': 'float64',\n",
    "    'air_quality_Sulphur_dioxide': 'float64',\n",
    "    'air_quality_PM2.5': 'float64',\n",
    "    'air_quality_PM10': 'float64',\n",
    "    'air_quality_us-epa-index': 'float64',\n",
    "    'air_quality_gb-defra-index': 'float64',\n",
    "    'sunrise': 'string',\n",
    "    'sunset': 'string',\n",
    "    'moonrise': 'string',\n",
    "    'moonset': 'string',\n",
    "    'moon_phase': 'string',\n",
    "    'moon_illumination': 'float64'\n",
    "}\n",
    "\n",
    "print(f\"Schema defined with {len(WEATHER_SCHEMA)} columns\")\n",
    "print(\"\\nData types distribution:\")\n",
    "type_counts = pd.Series(WEATHER_SCHEMA.values()).value_counts()\n",
    "for dtype, count in type_counts.items():\n",
    "    print(f\"  {dtype}: {count} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the schema explicitly is a best practice for several reasons:\n",
    "\n",
    "1. **Consistency**: Every chunk uses the same types, preventing merge errors\n",
    "2. **Memory efficiency**: Proper types use less memory than object types\n",
    "3. **Error detection**: Type mismatches surface immediately\n",
    "4. **Documentation**: The schema documents what data you expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_in_chunks(filepath, table_name, client, chunk_size=10000, schema=None):\n",
    "    \"\"\"\n",
    "    Load a large CSV file into a database table in chunks.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    filepath : str\n",
    "        Path to the CSV file\n",
    "    table_name : str\n",
    "        Name of the destination table\n",
    "    client : BigQuerySimulator\n",
    "        Database client for loading data\n",
    "    chunk_size : int\n",
    "        Number of rows per chunk (default: 10,000)\n",
    "    schema : dict\n",
    "        Column name to dtype mapping\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Statistics about the load operation\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    total_rows = 0\n",
    "    chunk_count = 0\n",
    "    \n",
    "    # Create a chunk iterator\n",
    "    chunks = pd.read_csv(\n",
    "        filepath,\n",
    "        chunksize=chunk_size,\n",
    "        dtype=schema,\n",
    "        encoding='utf-8'\n",
    "    )\n",
    "    \n",
    "    print(f\"Loading {filepath} into {table_name}\")\n",
    "    print(f\"Chunk size: {chunk_size:,} rows\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        chunk_count += 1\n",
    "        rows_in_chunk = len(chunk)\n",
    "        total_rows += rows_in_chunk\n",
    "        \n",
    "        # First chunk: create/replace table\n",
    "        # Subsequent chunks: append to table\n",
    "        if_exists = 'replace' if chunk_count == 1 else 'append'\n",
    "        \n",
    "        # Load the chunk\n",
    "        chunk.to_sql(table_name, client.connection, if_exists=if_exists, index=False)\n",
    "        \n",
    "        # Progress update\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = total_rows / elapsed if elapsed > 0 else 0\n",
    "        print(f\"  Chunk {chunk_count}: {rows_in_chunk:,} rows | Total: {total_rows:,} | Rate: {rate:,.0f} rows/sec\")\n",
    "    \n",
    "    # Final statistics\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    stats = {\n",
    "        'table_name': table_name,\n",
    "        'total_rows': total_rows,\n",
    "        'chunk_count': chunk_count,\n",
    "        'elapsed_seconds': round(elapsed_time, 2),\n",
    "        'rows_per_second': round(total_rows / elapsed_time, 0) if elapsed_time > 0 else 0\n",
    "    }\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Complete! {total_rows:,} rows loaded in {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `load_csv_in_chunks` function above implements the chunked loading strategy. Key design decisions:\n",
    "\n",
    "1. **`pd.read_csv` with `chunksize`**: Returns an iterator instead of a DataFrame, so only one chunk is in memory at a time\n",
    "\n",
    "2. **First chunk replaces, subsequent chunks append**: This ensures we start fresh but don't lose data from previous chunks\n",
    "\n",
    "3. **Progress tracking**: Real-time feedback on loading speed helps identify problems\n",
    "\n",
    "4. **Statistics return**: The function returns metrics that can be logged or monitored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../../Datasets/GlobalWeatherRepository.csv into global_weather\n",
      "Chunk size: 10,000 rows\n",
      "--------------------------------------------------\n",
      "  Chunk 1: 10,000 rows | Total: 10,000 | Rate: 40,070 rows/sec\n",
      "  Chunk 2: 10,000 rows | Total: 20,000 | Rate: 40,293 rows/sec\n",
      "  Chunk 3: 10,000 rows | Total: 30,000 | Rate: 40,403 rows/sec\n",
      "  Chunk 4: 10,000 rows | Total: 40,000 | Rate: 40,396 rows/sec\n",
      "  Chunk 5: 3,884 rows | Total: 43,884 | Rate: 39,705 rows/sec\n",
      "--------------------------------------------------\n",
      "Complete! 43,884 rows loaded in 1.11 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load our weather data using the chunked approach\n",
    "load_stats = load_csv_in_chunks(\n",
    "    filepath=DATA_PATH,\n",
    "    table_name='global_weather',\n",
    "    client=bq_client,\n",
    "    chunk_size=10000,\n",
    "    schema=WEATHER_SCHEMA\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows each chunk being processed with running totals. This visibility is important in production because:\n",
    "- You can estimate completion time\n",
    "- You can identify slow chunks that might indicate data issues\n",
    "- If the process fails, you know where it stopped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying loaded data...\n",
      "\n",
      "1. Row count:\n",
      "   43,884 rows in table\n",
      "\n",
      "2. Sample data:\n",
      "    country    location_name  temperature_celsius condition_text\n",
      "Afghanistan            Kabul                 26.6  Partly Cloudy\n",
      "    Albania           Tirana                 19.0  Partly cloudy\n",
      "    Algeria          Algiers                 23.0          Sunny\n",
      "    Andorra Andorra La Vella                  6.3  Light drizzle\n",
      "     Angola           Luanda                 26.0  Partly cloudy\n",
      "\n",
      "3. Countries in dataset:\n",
      "   210 unique countries\n"
     ]
    }
   ],
   "source": [
    "# Verify the load was successful\n",
    "print(\"Verifying loaded data...\")\n",
    "print(\"\\n1. Row count:\")\n",
    "result = bq_client.query(\"SELECT COUNT(*) as row_count FROM global_weather\")\n",
    "print(f\"   {result['row_count'].iloc[0]:,} rows in table\")\n",
    "\n",
    "print(\"\\n2. Sample data:\")\n",
    "sample = bq_client.query(\"SELECT country, location_name, temperature_celsius, condition_text FROM global_weather LIMIT 5\")\n",
    "print(sample.to_string(index=False))\n",
    "\n",
    "print(\"\\n3. Countries in dataset:\")\n",
    "countries = bq_client.query(\"SELECT COUNT(DISTINCT country) as country_count FROM global_weather\")\n",
    "print(f\"   {countries['country_count'].iloc[0]} unique countries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always verify your data after loading. The three checks above confirm:\n",
    "1. The expected number of rows arrived\n",
    "2. The data looks correct (spot check)\n",
    "3. Key dimensions are reasonable (country count)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Python Automation Scripts for ETL\n",
    "\n",
    "### From Notebook to Script\n",
    "\n",
    "Notebooks are great for exploration, but production ETL needs:\n",
    "- **Standalone scripts** that run without Jupyter\n",
    "- **Command-line arguments** for flexibility\n",
    "- **Configuration files** for environment-specific settings\n",
    "- **Proper logging** instead of print statements\n",
    "\n",
    "### Script Structure\n",
    "\n",
    "A well-organized ETL script follows this pattern:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                    ETL SCRIPT STRUCTURE                         │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│                                                                 │\n",
    "│  1. IMPORTS           - Libraries and modules                   │\n",
    "│  2. CONFIGURATION     - Load settings from file/environment     │\n",
    "│  3. LOGGING SETUP     - Configure logging handlers              │\n",
    "│  4. HELPER FUNCTIONS  - Reusable utility functions              │\n",
    "│  5. ETL FUNCTIONS     - Extract, Transform, Load                │\n",
    "│  6. MAIN FUNCTION     - Orchestrates the pipeline               │\n",
    "│  7. ENTRY POINT       - if __name__ == '__main__'               │\n",
    "│                                                                 │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETL Configuration:\n",
      "  source_path: ../../Datasets/GlobalWeatherRepository.csv\n",
      "  database_path: weather_warehouse.db\n",
      "  table_name: global_weather\n",
      "  chunk_size: 10000\n",
      "  max_retries: 3\n",
      "  retry_delay: 5\n",
      "  log_level: INFO\n"
     ]
    }
   ],
   "source": [
    "# Configuration management using a simple class\n",
    "\n",
    "class ETLConfig:\n",
    "    \"\"\"\n",
    "    Configuration class for ETL pipelines.\n",
    "    Loads settings from environment variables or defaults.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Data source settings\n",
    "        self.source_path = os.getenv('ETL_SOURCE_PATH', '../../Datasets/GlobalWeatherRepository.csv')\n",
    "        \n",
    "        # Database settings\n",
    "        self.database_path = os.getenv('ETL_DATABASE', 'weather_warehouse.db')\n",
    "        self.table_name = os.getenv('ETL_TABLE', 'global_weather')\n",
    "        \n",
    "        # Processing settings\n",
    "        self.chunk_size = int(os.getenv('ETL_CHUNK_SIZE', '10000'))\n",
    "        self.max_retries = int(os.getenv('ETL_MAX_RETRIES', '3'))\n",
    "        self.retry_delay = int(os.getenv('ETL_RETRY_DELAY', '5'))\n",
    "        \n",
    "        # Output settings\n",
    "        self.log_level = os.getenv('ETL_LOG_LEVEL', 'INFO')\n",
    "        self.log_file = os.getenv('ETL_LOG_FILE', 'etl_pipeline.log')\n",
    "    \n",
    "    def to_dict(self):\n",
    "        \"\"\"Return configuration as dictionary for logging.\"\"\"\n",
    "        return {\n",
    "            'source_path': self.source_path,\n",
    "            'database_path': self.database_path,\n",
    "            'table_name': self.table_name,\n",
    "            'chunk_size': self.chunk_size,\n",
    "            'max_retries': self.max_retries,\n",
    "            'retry_delay': self.retry_delay,\n",
    "            'log_level': self.log_level\n",
    "        }\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"ETLConfig({self.to_dict()})\"\n",
    "\n",
    "\n",
    "# Create and display configuration\n",
    "config = ETLConfig()\n",
    "print(\"ETL Configuration:\")\n",
    "for key, value in config.to_dict().items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ETLConfig` class above centralizes all configuration in one place. Benefits of this approach:\n",
    "\n",
    "1. **Environment-aware**: Uses `os.getenv()` to read from environment variables, with sensible defaults\n",
    "2. **Easy to modify**: Change settings without touching code\n",
    "3. **Self-documenting**: All settings are visible in one place\n",
    "4. **Testable**: Can create different configs for testing vs production\n",
    "\n",
    "In production, you might load from a YAML or JSON file:\n",
    "```python\n",
    "with open('config.yaml') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Scheduling and Task Automation\n",
    "\n",
    "### Why Schedule ETL Jobs?\n",
    "\n",
    "Data pipelines need to run automatically:\n",
    "- **Daily updates**: Refresh data every morning before business hours\n",
    "- **Hourly ingestion**: Stream near-real-time data\n",
    "- **Weekly aggregations**: Compute summary tables on weekends\n",
    "- **Event-triggered**: Run when new data arrives\n",
    "\n",
    "### Scheduling Options\n",
    "\n",
    "| Method | Platform | Best For |\n",
    "|--------|----------|----------|\n",
    "| Cron | Linux/Mac | Simple recurring jobs |\n",
    "| Task Scheduler | Windows | Simple Windows jobs |\n",
    "| Apache Airflow | Any | Complex workflows with dependencies |\n",
    "| Cloud Scheduler | GCP | Triggering cloud functions |\n",
    "| AWS EventBridge | AWS | Triggering Lambda functions |\n",
    "\n",
    "### Cron Syntax\n",
    "\n",
    "Cron uses a 5-field syntax to specify when jobs run:\n",
    "\n",
    "```\n",
    "┌───────────── minute (0 - 59)\n",
    "│ ┌───────────── hour (0 - 23)\n",
    "│ │ ┌───────────── day of month (1 - 31)\n",
    "│ │ │ ┌───────────── month (1 - 12)\n",
    "│ │ │ │ ┌───────────── day of week (0 - 6, Sunday = 0)\n",
    "│ │ │ │ │\n",
    "* * * * *  command to execute\n",
    "```\n",
    "\n",
    "**Examples:**\n",
    "```bash\n",
    "# Run every day at 6 AM\n",
    "0 6 * * * python /path/to/etl_script.py\n",
    "\n",
    "# Run every hour\n",
    "0 * * * * python /path/to/etl_script.py\n",
    "\n",
    "# Run Monday at midnight\n",
    "0 0 * * 1 python /path/to/etl_script.py\n",
    "\n",
    "# Run every 15 minutes\n",
    "*/15 * * * * python /path/to/etl_script.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schedule Recommendations:\n",
      "\n",
      "Daily dashboard refresh:\n",
      "  Freshness: 24h, Processing: 30min\n",
      "  Schedule: 0 6 * * *  # Daily at 6 AM\n",
      "\n",
      "Near-real-time monitoring:\n",
      "  Freshness: 4h, Processing: 15min\n",
      "  Schedule: 0 */4 * * *  # Every 4 hours\n",
      "\n",
      "Weekly aggregation report:\n",
      "  Freshness: 168h, Processing: 60min\n",
      "  Schedule: 0 6 * * 1  # Weekly on Monday at 6 AM\n",
      "\n",
      "Hourly data feed:\n",
      "  Freshness: 1h, Processing: 5min\n",
      "  Schedule: 0 * * * *  # Every hour\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to generate cron schedule recommendations\n",
    "\n",
    "def recommend_schedule(data_freshness_hours, processing_time_minutes):\n",
    "    \"\"\"\n",
    "    Recommend a cron schedule based on data requirements.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_freshness_hours : int\n",
    "        Maximum acceptable age of data in hours\n",
    "    processing_time_minutes : int\n",
    "        Estimated time to run the ETL job\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str : Recommended cron expression\n",
    "    \"\"\"\n",
    "    # Add buffer for processing time\n",
    "    effective_interval = data_freshness_hours - (processing_time_minutes / 60)\n",
    "    \n",
    "    if effective_interval <= 0:\n",
    "        return \"ERROR: Processing time exceeds freshness requirement\"\n",
    "    \n",
    "    if effective_interval <= 1:\n",
    "        return \"0 * * * *  # Every hour\"\n",
    "    elif effective_interval <= 4:\n",
    "        return \"0 */4 * * *  # Every 4 hours\"\n",
    "    elif effective_interval <= 12:\n",
    "        return \"0 6,18 * * *  # Twice daily (6 AM and 6 PM)\"\n",
    "    elif effective_interval <= 24:\n",
    "        return \"0 6 * * *  # Daily at 6 AM\"\n",
    "    else:\n",
    "        return \"0 6 * * 1  # Weekly on Monday at 6 AM\"\n",
    "\n",
    "\n",
    "# Example: Data must be no more than 24 hours old, job takes 30 minutes\n",
    "print(\"Schedule Recommendations:\\n\")\n",
    "\n",
    "scenarios = [\n",
    "    (24, 30, \"Daily dashboard refresh\"),\n",
    "    (4, 15, \"Near-real-time monitoring\"),\n",
    "    (168, 60, \"Weekly aggregation report\"),\n",
    "    (1, 5, \"Hourly data feed\"),\n",
    "]\n",
    "\n",
    "for freshness, processing, description in scenarios:\n",
    "    schedule = recommend_schedule(freshness, processing)\n",
    "    print(f\"{description}:\")\n",
    "    print(f\"  Freshness: {freshness}h, Processing: {processing}min\")\n",
    "    print(f\"  Schedule: {schedule}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `recommend_schedule` function helps determine appropriate run frequencies. The key insight is that your job must complete before the next scheduled run, and the data must remain fresh enough for business needs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Handling and Logging in Production\n",
    "\n",
    "### Why Logging Matters\n",
    "\n",
    "Print statements work in notebooks, but production ETL needs proper logging:\n",
    "\n",
    "| Print Statements | Proper Logging |\n",
    "|-----------------|----------------|\n",
    "| Goes to stdout only | Can go to files, databases, cloud services |\n",
    "| No severity levels | DEBUG, INFO, WARNING, ERROR, CRITICAL |\n",
    "| No timestamps | Automatic timestamps |\n",
    "| Lost when terminal closes | Persisted for analysis |\n",
    "| Hard to filter | Easy to filter by level |\n",
    "\n",
    "### Logging Levels\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                      LOGGING LEVELS                             │\n",
    "├───────────┬─────────────────────────────────────────────────────┤\n",
    "│  DEBUG    │  Detailed diagnostic info (variable values, etc.)  │\n",
    "├───────────┼─────────────────────────────────────────────────────┤\n",
    "│  INFO     │  Confirmation that things work as expected         │\n",
    "├───────────┼─────────────────────────────────────────────────────┤\n",
    "│  WARNING  │  Something unexpected, but not an error            │\n",
    "├───────────┼─────────────────────────────────────────────────────┤\n",
    "│  ERROR    │  Serious problem, some functionality failed        │\n",
    "├───────────┼─────────────────────────────────────────────────────┤\n",
    "│  CRITICAL │  Program cannot continue                           │\n",
    "└───────────┴─────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO     | This is an INFO message - visible in console\n",
      "WARNING  | This is a WARNING message\n",
      "ERROR    | This is an ERROR message\n"
     ]
    }
   ],
   "source": [
    "# Set up production-grade logging\n",
    "\n",
    "def setup_logging(log_file='etl_pipeline.log', level=logging.INFO):\n",
    "    \"\"\"\n",
    "    Configure logging for ETL pipeline.\n",
    "    \n",
    "    Creates two handlers:\n",
    "    1. File handler: Writes all logs to file\n",
    "    2. Console handler: Shows INFO and above in terminal\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    log_file : str\n",
    "        Path to log file\n",
    "    level : int\n",
    "        Logging level (e.g., logging.INFO)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    logging.Logger : Configured logger\n",
    "    \"\"\"\n",
    "    # Create logger\n",
    "    logger = logging.getLogger('etl_pipeline')\n",
    "    logger.setLevel(level)\n",
    "    \n",
    "    # Clear any existing handlers (important for Jupyter)\n",
    "    logger.handlers = []\n",
    "    \n",
    "    # Create formatters\n",
    "    detailed_format = logging.Formatter(\n",
    "        '%(asctime)s | %(levelname)-8s | %(funcName)s | %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "    simple_format = logging.Formatter(\n",
    "        '%(levelname)-8s | %(message)s'\n",
    "    )\n",
    "    \n",
    "    # File handler - captures everything\n",
    "    file_handler = logging.FileHandler(log_file)\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    file_handler.setFormatter(detailed_format)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    # Console handler - shows INFO and above\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    console_handler.setFormatter(simple_format)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "\n",
    "# Create our logger\n",
    "logger = setup_logging()\n",
    "\n",
    "# Demonstrate logging levels\n",
    "logger.debug(\"This is a DEBUG message - only in file\")\n",
    "logger.info(\"This is an INFO message - visible in console\")\n",
    "logger.warning(\"This is a WARNING message\")\n",
    "logger.error(\"This is an ERROR message\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `setup_logging` function creates a professional logging configuration:\n",
    "\n",
    "1. **Two handlers**: File gets everything (for debugging), console shows INFO and above (for monitoring)\n",
    "2. **Detailed timestamps**: Essential for debugging \"when did this happen?\"\n",
    "3. **Function names**: Helps locate where messages originate\n",
    "4. **Level padding**: Aligned output for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing retry mechanism with unreliable function...\n",
      "Success: {'status': 'success', 'data': [1, 2, 3]}\n"
     ]
    }
   ],
   "source": [
    "# Retry decorator with exponential backoff\n",
    "\n",
    "import functools\n",
    "\n",
    "def retry_with_backoff(max_retries=3, base_delay=1, max_delay=60):\n",
    "    \"\"\"\n",
    "    Decorator that retries a function with exponential backoff.\n",
    "    \n",
    "    Exponential backoff means each retry waits longer:\n",
    "    - Attempt 1: immediate\n",
    "    - Attempt 2: wait base_delay seconds\n",
    "    - Attempt 3: wait base_delay * 2 seconds\n",
    "    - Attempt 4: wait base_delay * 4 seconds\n",
    "    - etc.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    max_retries : int\n",
    "        Maximum number of retry attempts\n",
    "    base_delay : int\n",
    "        Initial delay in seconds\n",
    "    max_delay : int\n",
    "        Maximum delay between retries\n",
    "    \"\"\"\n",
    "    def decorator(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            last_exception = None\n",
    "            \n",
    "            for attempt in range(max_retries + 1):\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    last_exception = e\n",
    "                    \n",
    "                    if attempt < max_retries:\n",
    "                        # Calculate delay with exponential backoff\n",
    "                        delay = min(base_delay * (2 ** attempt), max_delay)\n",
    "                        logger.warning(\n",
    "                            f\"Attempt {attempt + 1} failed: {e}. \"\n",
    "                            f\"Retrying in {delay} seconds...\"\n",
    "                        )\n",
    "                        time.sleep(delay)\n",
    "                    else:\n",
    "                        logger.error(\n",
    "                            f\"All {max_retries + 1} attempts failed. \"\n",
    "                            f\"Last error: {e}\"\n",
    "                        )\n",
    "            \n",
    "            raise last_exception\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "\n",
    "# Example: A function that might fail\n",
    "@retry_with_backoff(max_retries=3, base_delay=1)\n",
    "def unreliable_api_call(success_probability=0.3):\n",
    "    \"\"\"\n",
    "    Simulates an unreliable API that fails randomly.\n",
    "    \"\"\"\n",
    "    import random\n",
    "    if random.random() > success_probability:\n",
    "        raise ConnectionError(\"API server unavailable\")\n",
    "    return {\"status\": \"success\", \"data\": [1, 2, 3]}\n",
    "\n",
    "\n",
    "# Test the retry mechanism\n",
    "print(\"Testing retry mechanism with unreliable function...\")\n",
    "try:\n",
    "    result = unreliable_api_call(success_probability=0.7)\n",
    "    print(f\"Success: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"Final failure: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `retry_with_backoff` decorator is a production essential. Exponential backoff is important because:\n",
    "\n",
    "1. **Transient failures recover**: Network glitches often resolve in seconds\n",
    "2. **Don't overwhelm failing systems**: Increasing delays give servers time to recover\n",
    "3. **Automatic recovery**: The pipeline self-heals without manual intervention\n",
    "4. **Clean code**: Error handling logic is separated from business logic\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Monitoring Pipeline Health\n",
    "\n",
    "### What to Monitor\n",
    "\n",
    "A healthy ETL pipeline should track:\n",
    "\n",
    "1. **Execution metrics**: Runtime, rows processed, success/failure\n",
    "2. **Data quality metrics**: Null rates, duplicates, range violations\n",
    "3. **Resource metrics**: Memory usage, disk space, API quotas\n",
    "4. **Business metrics**: Data freshness, completeness, accuracy\n",
    "\n",
    "### Simple Monitoring Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO     | Pipeline 'weather_etl' started\n",
      "INFO     | Pipeline completed successfully in 0.11s\n",
      "INFO     | Pipeline 'weather_etl' started\n",
      "INFO     | Pipeline completed successfully in 0.11s\n",
      "INFO     | Pipeline 'weather_etl' started\n",
      "INFO     | Pipeline completed successfully in 0.11s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pipeline Summary:\n",
      "  total_runs: 3\n",
      "  successful_runs: 3\n",
      "  failed_runs: 0\n",
      "  avg_duration_seconds: 0.11\n",
      "  max_duration_seconds: 0.11\n",
      "  success_rate: 100.0\n"
     ]
    }
   ],
   "source": [
    "class PipelineMonitor:\n",
    "    \"\"\"\n",
    "    Monitors ETL pipeline health and collects metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pipeline_name):\n",
    "        self.pipeline_name = pipeline_name\n",
    "        self.metrics = []\n",
    "        self.start_time = None\n",
    "        self.current_run = {}\n",
    "    \n",
    "    def start_run(self):\n",
    "        \"\"\"Mark the beginning of a pipeline run.\"\"\"\n",
    "        self.start_time = datetime.now()\n",
    "        self.current_run = {\n",
    "            'pipeline': self.pipeline_name,\n",
    "            'start_time': self.start_time.isoformat(),\n",
    "            'status': 'running'\n",
    "        }\n",
    "        logger.info(f\"Pipeline '{self.pipeline_name}' started\")\n",
    "    \n",
    "    def record_metric(self, name, value):\n",
    "        \"\"\"Record a metric for the current run.\"\"\"\n",
    "        self.current_run[name] = value\n",
    "        logger.debug(f\"Metric recorded: {name} = {value}\")\n",
    "    \n",
    "    def end_run(self, status='success', error_message=None):\n",
    "        \"\"\"Mark the end of a pipeline run.\"\"\"\n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - self.start_time).total_seconds()\n",
    "        \n",
    "        self.current_run.update({\n",
    "            'end_time': end_time.isoformat(),\n",
    "            'duration_seconds': round(duration, 2),\n",
    "            'status': status\n",
    "        })\n",
    "        \n",
    "        if error_message:\n",
    "            self.current_run['error'] = error_message\n",
    "        \n",
    "        self.metrics.append(self.current_run.copy())\n",
    "        \n",
    "        if status == 'success':\n",
    "            logger.info(f\"Pipeline completed successfully in {duration:.2f}s\")\n",
    "        else:\n",
    "            logger.error(f\"Pipeline failed after {duration:.2f}s: {error_message}\")\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get summary statistics across all runs.\"\"\"\n",
    "        if not self.metrics:\n",
    "            return \"No runs recorded\"\n",
    "        \n",
    "        df = pd.DataFrame(self.metrics)\n",
    "        \n",
    "        summary = {\n",
    "            'total_runs': len(df),\n",
    "            'successful_runs': len(df[df['status'] == 'success']),\n",
    "            'failed_runs': len(df[df['status'] == 'failed']),\n",
    "            'avg_duration_seconds': df['duration_seconds'].mean(),\n",
    "            'max_duration_seconds': df['duration_seconds'].max(),\n",
    "            'success_rate': len(df[df['status'] == 'success']) / len(df) * 100\n",
    "        }\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def export_metrics(self, filepath):\n",
    "        \"\"\"Export metrics to JSON file.\"\"\"\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(self.metrics, f, indent=2)\n",
    "        logger.info(f\"Metrics exported to {filepath}\")\n",
    "\n",
    "\n",
    "# Demonstrate the monitor\n",
    "monitor = PipelineMonitor('weather_etl')\n",
    "\n",
    "# Simulate a few runs\n",
    "for i in range(3):\n",
    "    monitor.start_run()\n",
    "    monitor.record_metric('rows_processed', 43884)\n",
    "    monitor.record_metric('null_rate', 0.02)\n",
    "    time.sleep(0.1)  # Simulate work\n",
    "    monitor.end_run(status='success')\n",
    "\n",
    "print(\"\\nPipeline Summary:\")\n",
    "for key, value in monitor.get_summary().items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `PipelineMonitor` class provides a foundation for monitoring. In production, you would extend this to:\n",
    "\n",
    "1. **Send alerts**: Email or Slack when failures occur\n",
    "2. **Store metrics**: Write to a database for historical analysis\n",
    "3. **Create dashboards**: Visualize trends over time\n",
    "4. **Set thresholds**: Auto-alert when metrics exceed limits\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Complete Automated ETL Example\n",
    "\n",
    "Now let's put everything together into a production-ready ETL pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherETLPipeline:\n",
    "    \"\"\"\n",
    "    Complete ETL pipeline for weather data.\n",
    "    \n",
    "    This class demonstrates production-ready patterns:\n",
    "    - Configuration management\n",
    "    - Logging throughout\n",
    "    - Error handling with retries\n",
    "    - Monitoring and metrics\n",
    "    - Data validation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initialize the pipeline with configuration.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        config : ETLConfig\n",
    "            Configuration object with all settings\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.monitor = PipelineMonitor('weather_etl')\n",
    "        self.client = None\n",
    "        \n",
    "        logger.info(f\"Pipeline initialized with config: {config.to_dict()}\")\n",
    "    \n",
    "    def connect(self):\n",
    "        \"\"\"Establish database connection.\"\"\"\n",
    "        logger.info(\"Connecting to database...\")\n",
    "        self.client = BigQuerySimulator(self.config.database_path)\n",
    "    \n",
    "    def extract(self):\n",
    "        \"\"\"\n",
    "        Extract phase: Read and validate source data.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame : Extracted data\n",
    "        \"\"\"\n",
    "        logger.info(f\"Extracting data from {self.config.source_path}\")\n",
    "        \n",
    "        # Verify source file exists\n",
    "        if not os.path.exists(self.config.source_path):\n",
    "            raise FileNotFoundError(f\"Source file not found: {self.config.source_path}\")\n",
    "        \n",
    "        # Read the data\n",
    "        df = pd.read_csv(self.config.source_path, dtype=WEATHER_SCHEMA)\n",
    "        \n",
    "        logger.info(f\"Extracted {len(df):,} rows, {len(df.columns)} columns\")\n",
    "        self.monitor.record_metric('extracted_rows', len(df))\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def transform(self, df):\n",
    "        \"\"\"\n",
    "        Transform phase: Clean and enhance data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pd.DataFrame\n",
    "            Raw extracted data\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame : Transformed data\n",
    "        \"\"\"\n",
    "        logger.info(\"Starting transformation...\")\n",
    "        \n",
    "        # Make a copy to avoid modifying original\n",
    "        df_clean = df.copy()\n",
    "        \n",
    "        # 1. Parse datetime\n",
    "        logger.debug(\"Parsing datetime column\")\n",
    "        df_clean['last_updated_dt'] = pd.to_datetime(df_clean['last_updated'])\n",
    "        \n",
    "        # 2. Extract date components\n",
    "        df_clean['update_date'] = df_clean['last_updated_dt'].dt.date\n",
    "        df_clean['update_hour'] = df_clean['last_updated_dt'].dt.hour\n",
    "        \n",
    "        # 3. Calculate derived metrics\n",
    "        logger.debug(\"Calculating derived metrics\")\n",
    "        df_clean['temp_feels_diff'] = (\n",
    "            df_clean['temperature_celsius'] - df_clean['feels_like_celsius']\n",
    "        ).round(2)\n",
    "        \n",
    "        # 4. Categorize wind speed\n",
    "        df_clean['wind_category'] = pd.cut(\n",
    "            df_clean['wind_kph'],\n",
    "            bins=[0, 10, 30, 50, 100, float('inf')],\n",
    "            labels=['Calm', 'Light', 'Moderate', 'Strong', 'Severe']\n",
    "        )\n",
    "        \n",
    "        # 5. Add ETL metadata\n",
    "        df_clean['_etl_timestamp'] = datetime.now().isoformat()\n",
    "        df_clean['_etl_source'] = self.config.source_path\n",
    "        \n",
    "        # Record data quality metrics\n",
    "        null_rate = df_clean.isnull().sum().sum() / (len(df_clean) * len(df_clean.columns))\n",
    "        self.monitor.record_metric('null_rate', round(null_rate, 4))\n",
    "        self.monitor.record_metric('transformed_rows', len(df_clean))\n",
    "        \n",
    "        logger.info(f\"Transformation complete. Null rate: {null_rate:.2%}\")\n",
    "        \n",
    "        return df_clean\n",
    "    \n",
    "    def validate(self, df):\n",
    "        \"\"\"\n",
    "        Validate data quality before loading.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pd.DataFrame\n",
    "            Transformed data to validate\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        bool : True if validation passes\n",
    "        \"\"\"\n",
    "        logger.info(\"Running validation checks...\")\n",
    "        \n",
    "        issues = []\n",
    "        \n",
    "        # Check 1: Row count\n",
    "        if len(df) == 0:\n",
    "            issues.append(\"ERROR: DataFrame is empty\")\n",
    "        \n",
    "        # Check 2: Required columns exist\n",
    "        required_cols = ['country', 'location_name', 'temperature_celsius']\n",
    "        missing_cols = [c for c in required_cols if c not in df.columns]\n",
    "        if missing_cols:\n",
    "            issues.append(f\"ERROR: Missing required columns: {missing_cols}\")\n",
    "        \n",
    "        # Check 3: Temperature range (reasonable bounds)\n",
    "        temp_min = df['temperature_celsius'].min()\n",
    "        temp_max = df['temperature_celsius'].max()\n",
    "        if temp_min < -90 or temp_max > 60:\n",
    "            issues.append(f\"WARNING: Temperature out of range: [{temp_min}, {temp_max}]\")\n",
    "        \n",
    "        # Check 4: No duplicate location-time combinations\n",
    "        dup_count = df.duplicated(subset=['country', 'location_name', 'last_updated']).sum()\n",
    "        if dup_count > 0:\n",
    "            issues.append(f\"WARNING: {dup_count} duplicate records found\")\n",
    "        \n",
    "        # Report issues\n",
    "        for issue in issues:\n",
    "            if issue.startswith('ERROR'):\n",
    "                logger.error(issue)\n",
    "            else:\n",
    "                logger.warning(issue)\n",
    "        \n",
    "        # Only fail on errors, not warnings\n",
    "        errors = [i for i in issues if i.startswith('ERROR')]\n",
    "        \n",
    "        self.monitor.record_metric('validation_issues', len(issues))\n",
    "        self.monitor.record_metric('validation_errors', len(errors))\n",
    "        \n",
    "        return len(errors) == 0\n",
    "    \n",
    "    def load(self, df):\n",
    "        \"\"\"\n",
    "        Load phase: Write data to destination.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pd.DataFrame\n",
    "            Validated data to load\n",
    "        \"\"\"\n",
    "        logger.info(f\"Loading {len(df):,} rows to {self.config.table_name}\")\n",
    "        \n",
    "        # Load in chunks for large datasets\n",
    "        self.client.load_table_from_dataframe(df, self.config.table_name)\n",
    "        \n",
    "        self.monitor.record_metric('loaded_rows', len(df))\n",
    "        logger.info(\"Load complete\")\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Execute the complete ETL pipeline.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Run statistics\n",
    "        \"\"\"\n",
    "        self.monitor.start_run()\n",
    "        \n",
    "        try:\n",
    "            # Connect\n",
    "            self.connect()\n",
    "            \n",
    "            # Extract\n",
    "            df_raw = self.extract()\n",
    "            \n",
    "            # Transform\n",
    "            df_transformed = self.transform(df_raw)\n",
    "            \n",
    "            # Validate\n",
    "            if not self.validate(df_transformed):\n",
    "                raise ValueError(\"Data validation failed\")\n",
    "            \n",
    "            # Load\n",
    "            self.load(df_transformed)\n",
    "            \n",
    "            # Success\n",
    "            self.monitor.end_run(status='success')\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.monitor.end_run(status='failed', error_message=str(e))\n",
    "            raise\n",
    "        \n",
    "        finally:\n",
    "            if self.client:\n",
    "                self.client.close()\n",
    "        \n",
    "        return self.monitor.current_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `WeatherETLPipeline` class brings together all the concepts from this lecture:\n",
    "\n",
    "1. **Configuration**: Uses `ETLConfig` for all settings\n",
    "2. **Logging**: Every step is logged with appropriate levels\n",
    "3. **Monitoring**: Metrics are recorded throughout\n",
    "4. **Validation**: Data quality is checked before loading\n",
    "5. **Error handling**: Failures are caught and reported\n",
    "6. **Clean structure**: Extract, Transform, Load are separate methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO     | Pipeline initialized with config: {'source_path': '../../Datasets/GlobalWeatherRepository.csv', 'database_path': 'weather_warehouse.db', 'table_name': 'global_weather', 'chunk_size': 10000, 'max_retries': 3, 'retry_delay': 5, 'log_level': 'INFO'}\n",
      "INFO     | Pipeline 'weather_etl' started\n",
      "INFO     | Connecting to database...\n",
      "INFO     | Extracting data from ../../Datasets/GlobalWeatherRepository.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXECUTING WEATHER ETL PIPELINE\n",
      "============================================================\n",
      "Connected to simulated BigQuery (SQLite: weather_warehouse.db)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO     | Extracted 43,884 rows, 41 columns\n",
      "INFO     | Starting transformation...\n",
      "INFO     | Transformation complete. Null rate: 0.00%\n",
      "INFO     | Running validation checks...\n",
      "INFO     | Loading 43,884 rows to global_weather\n",
      "INFO     | Load complete\n",
      "INFO     | Pipeline completed successfully in 1.41s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 43,884 rows into global_weather\n",
      "Connection closed.\n",
      "\n",
      "============================================================\n",
      "PIPELINE RESULT\n",
      "============================================================\n",
      "  pipeline: weather_etl\n",
      "  start_time: 2026-01-27T21:58:56.475142\n",
      "  status: success\n",
      "  extracted_rows: 43884\n",
      "  null_rate: 0.0\n",
      "  transformed_rows: 43884\n",
      "  validation_issues: 0\n",
      "  validation_errors: 0\n",
      "  loaded_rows: 43884\n",
      "  end_time: 2026-01-27T21:58:57.885971\n",
      "  duration_seconds: 1.41\n"
     ]
    }
   ],
   "source": [
    "# Run the complete pipeline\n",
    "print(\"=\"*60)\n",
    "print(\"EXECUTING WEATHER ETL PIPELINE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create configuration\n",
    "config = ETLConfig()\n",
    "\n",
    "# Create and run pipeline\n",
    "pipeline = WeatherETLPipeline(config)\n",
    "result = pipeline.run()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PIPELINE RESULT\")\n",
    "print(\"=\"*60)\n",
    "for key, value in result.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline output shows each phase executing with proper logging. In production, this output would go to log files and monitoring dashboards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying loaded data...\n",
      "\n",
      "Connected to simulated BigQuery (SQLite: weather_warehouse.db)\n",
      "Total rows: 43,884\n",
      "\n",
      "Sample data with derived columns:\n",
      "            country    location_name  temperature_celsius  feels_like_celsius  temp_feels_diff wind_category\n",
      "        Afghanistan            Kabul                 26.6                25.3              1.3         Light\n",
      "            Albania           Tirana                 19.0                19.0              0.0         Light\n",
      "            Algeria          Algiers                 23.0                24.6             -1.6         Light\n",
      "            Andorra Andorra La Vella                  6.3                 3.8              2.5         Light\n",
      "             Angola           Luanda                 26.0                28.7             -2.7         Light\n",
      "Antigua and Barbuda     Saint John's                 26.0                28.2             -2.2          Calm\n",
      "          Argentina     Buenos Aires                  8.0                 7.1              0.9          Calm\n",
      "            Armenia          Yerevan                 19.0                19.0              0.0          Calm\n",
      "          Australia         Canberra                  9.0                 9.1             -0.1          Calm\n",
      "            Austria           Vienna                 16.0                16.0              0.0         Light\n",
      "\n",
      "Wind category distribution:\n",
      "wind_category  count\n",
      "        Light  23642\n",
      "         Calm  18422\n",
      "     Moderate   1755\n",
      "       Strong     60\n",
      "       Severe      5\n",
      "Connection closed.\n"
     ]
    }
   ],
   "source": [
    "# Verify the loaded data\n",
    "print(\"Verifying loaded data...\\n\")\n",
    "\n",
    "# Reconnect to check data\n",
    "verify_client = BigQuerySimulator(config.database_path)\n",
    "\n",
    "# Check row count\n",
    "count_result = verify_client.query(f\"SELECT COUNT(*) as cnt FROM {config.table_name}\")\n",
    "print(f\"Total rows: {count_result['cnt'].iloc[0]:,}\")\n",
    "\n",
    "# Check sample with new columns\n",
    "print(\"\\nSample data with derived columns:\")\n",
    "sample = verify_client.query(f\"\"\"\n",
    "    SELECT \n",
    "        country,\n",
    "        location_name,\n",
    "        temperature_celsius,\n",
    "        feels_like_celsius,\n",
    "        temp_feels_diff,\n",
    "        wind_category\n",
    "    FROM {config.table_name}\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "print(sample.to_string(index=False))\n",
    "\n",
    "# Check wind category distribution\n",
    "print(\"\\nWind category distribution:\")\n",
    "wind_dist = verify_client.query(f\"\"\"\n",
    "    SELECT \n",
    "        wind_category,\n",
    "        COUNT(*) as count\n",
    "    FROM {config.table_name}\n",
    "    GROUP BY wind_category\n",
    "    ORDER BY count DESC\n",
    "\"\"\")\n",
    "print(wind_dist.to_string(index=False))\n",
    "\n",
    "verify_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The verification confirms our pipeline worked correctly. The derived columns (`temp_feels_diff`, `wind_category`) are present and the data is properly categorized.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Key Takeaways\n",
    "\n",
    "### 1. Cloud Data Warehouses\n",
    "- Separate storage and compute for scalability\n",
    "- Pay-per-query pricing model\n",
    "- BigQuery: serverless, no infrastructure to manage\n",
    "\n",
    "### 2. Loading Large Datasets\n",
    "- Use chunked loading for memory efficiency\n",
    "- Define schemas explicitly for consistency\n",
    "- Always verify after loading\n",
    "\n",
    "### 3. Automation Scripts\n",
    "- Use configuration classes for flexibility\n",
    "- Separate Extract, Transform, Load phases\n",
    "- Keep business logic testable\n",
    "\n",
    "### 4. Scheduling\n",
    "- Cron for simple recurring jobs\n",
    "- Consider processing time vs freshness requirements\n",
    "- Use Airflow for complex dependencies\n",
    "\n",
    "### 5. Error Handling\n",
    "- Retry with exponential backoff\n",
    "- Proper logging (not print statements)\n",
    "- Validate data before loading\n",
    "\n",
    "### 6. Monitoring\n",
    "- Track execution metrics\n",
    "- Record data quality metrics\n",
    "- Export for dashboards and alerting\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "### Books\n",
    "- Densmore, J. (2021). *Data Pipelines Pocket Reference*. O'Reilly Media.\n",
    "- Reis, J., & Housley, M. (2022). *Fundamentals of Data Engineering*. O'Reilly Media.\n",
    "- Kleppmann, M. (2017). *Designing Data-Intensive Applications*. O'Reilly Media.\n",
    "\n",
    "### Documentation\n",
    "- [Google BigQuery Documentation](https://cloud.google.com/bigquery/docs)\n",
    "- [Python logging module](https://docs.python.org/3/library/logging.html)\n",
    "- [pandas read_csv chunking](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)\n",
    "\n",
    "### Best Practices\n",
    "- [Google Cloud Architecture: ETL best practices](https://cloud.google.com/architecture/building-production-ready-data-pipelines-using-dataflow)\n",
    "- [AWS: ETL best practices](https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/etl-best-practices.html)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "### Exercise 1: Add Email Notifications\n",
    "Extend the `PipelineMonitor` class to send an email when a pipeline fails. Use Python's `smtplib` library.\n",
    "\n",
    "### Exercise 2: Incremental Loading\n",
    "Modify the pipeline to only load records newer than the most recent record in the database (incremental load vs full refresh).\n",
    "\n",
    "### Exercise 3: Data Quality Dashboard\n",
    "Create a function that reads the metrics JSON file and generates a summary report with:\n",
    "- Success rate over time\n",
    "- Average runtime trend\n",
    "- Most common failure reasons\n",
    "\n",
    "### Exercise 4: Multi-Source Pipeline\n",
    "Extend the pipeline to extract from multiple CSV files and merge them before loading.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Class: Data Cleaning I\n",
    "\n",
    "In Lecture 9, we will cover:\n",
    "- Understanding dirty data patterns\n",
    "- Data type standardization\n",
    "- Date and time parsing\n",
    "- String cleaning and normalization\n",
    "- Deduplication strategies\n",
    "\n",
    "We will continue using the GlobalWeatherRepository and introduce the AirQualityUCI dataset, which has intentional data quality issues for us to fix.\n",
    "\n",
    "---\n",
    "\n",
    "*End of Lecture 8*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ubc-mfre",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
