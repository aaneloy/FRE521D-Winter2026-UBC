{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FRE 521D: Data Analytics in Climate, Food and Environment\n",
    "## Lecture 5: Python Wrangling I - Tidy Data, Types, and Validation\n",
    "\n",
    "**Date:** Wednesday, January 21, 2026  \n",
    "**Instructor:** Asif Ahmed Neloy  \n",
    "**Program:** UBC Master of Food and Resource Economics\n",
    "\n",
    "---\n",
    "\n",
    "### Today's Agenda\n",
    "\n",
    "1. What is Tidy Data? Why Does It Matter?\n",
    "2. Column Typing and Type Coercion\n",
    "3. Reshaping Data: Wide to Long\n",
    "4. Missing Data Strategies\n",
    "5. Validation Checks: Ranges, Nulls, and Keys\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 2.3.3\n",
      "NumPy version: 2.2.6\n",
      "Lecture date: 2026-01-06\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Lecture date: {datetime.now().strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. What is Tidy Data?\n",
    "\n",
    "### The Concept\n",
    "\n",
    "**Tidy data** is a standard way of organizing data that makes analysis easier. The concept comes from Hadley Wickham's influential paper.\n",
    "\n",
    "### The Three Rules of Tidy Data\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────────────────────┐\n",
    "│                     TIDY DATA PRINCIPLES                       │\n",
    "├────────────────────────────────────────────────────────────────┤\n",
    "│                                                                │\n",
    "│  1. Each VARIABLE has its own COLUMN                          │\n",
    "│                                                                │\n",
    "│  2. Each OBSERVATION has its own ROW                          │\n",
    "│                                                                │\n",
    "│  3. Each VALUE has its own CELL                               │\n",
    "│                                                                │\n",
    "└────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Why Tidy Data Matters\n",
    "\n",
    "- **Consistency**: One format works with all tools\n",
    "- **Simplicity**: Standard verbs (filter, group, summarize) work predictably\n",
    "- **Vectorization**: Operations apply to entire columns efficiently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Messy vs Tidy\n",
    "\n",
    "Let's look at the same data in messy and tidy formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MESSY DATA (years as columns):\n",
      "  country         indicator   2020   2021   2022\n",
      "0  Canada  wheat_production  35183  22296  34335\n",
      "1     USA  wheat_production  49691  44790  44900\n",
      "2  Mexico  wheat_production   3115   3024   3195\n",
      "\n",
      "Shape: (3, 5)\n"
     ]
    }
   ],
   "source": [
    "# MESSY DATA: Years as columns (wide format)\n",
    "# This is how data often comes from spreadsheets\n",
    "\n",
    "messy_data = pd.DataFrame({\n",
    "    'country': ['Canada', 'USA', 'Mexico'],\n",
    "    'indicator': ['wheat_production', 'wheat_production', 'wheat_production'],\n",
    "    '2020': [35183, 49691, 3115],\n",
    "    '2021': [22296, 44790, 3024],\n",
    "    '2022': [34335, 44900, 3195]\n",
    "})\n",
    "\n",
    "print(\"MESSY DATA (years as columns):\")\n",
    "print(messy_data)\n",
    "print(f\"\\nShape: {messy_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIDY DATA (each row is one country-year):\n",
      "  country  year  wheat_production\n",
      "0  Canada  2020             35183\n",
      "1  Canada  2021             22296\n",
      "2  Canada  2022             34335\n",
      "3     USA  2020             49691\n",
      "4     USA  2021             44790\n",
      "5     USA  2022             44900\n",
      "6  Mexico  2020              3115\n",
      "7  Mexico  2021              3024\n",
      "8  Mexico  2022              3195\n",
      "\n",
      "Shape: (9, 3)\n"
     ]
    }
   ],
   "source": [
    "# TIDY DATA: Each row is one observation (country-year)\n",
    "\n",
    "tidy_data = pd.DataFrame({\n",
    "    'country': ['Canada', 'Canada', 'Canada', 'USA', 'USA', 'USA', 'Mexico', 'Mexico', 'Mexico'],\n",
    "    'year': [2020, 2021, 2022, 2020, 2021, 2022, 2020, 2021, 2022],\n",
    "    'wheat_production': [35183, 22296, 34335, 49691, 44790, 44900, 3115, 3024, 3195]\n",
    "})\n",
    "\n",
    "print(\"TIDY DATA (each row is one country-year):\")\n",
    "print(tidy_data)\n",
    "print(f\"\\nShape: {tidy_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With TIDY data - Total production in 2021:\n",
      "  70,110 thousand tonnes\n",
      "\n",
      "With TIDY data - Average by country:\n",
      "country\n",
      "Canada    30604.666667\n",
      "Mexico     3111.333333\n",
      "USA       46460.333333\n",
      "Name: wheat_production, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Why tidy is better: Easy to filter, group, and analyze\n",
    "\n",
    "# Question: What was total wheat production in 2021?\n",
    "print(\"With TIDY data - Total production in 2021:\")\n",
    "result = tidy_data[tidy_data['year'] == 2021]['wheat_production'].sum()\n",
    "print(f\"  {result:,} thousand tonnes\")\n",
    "\n",
    "# Question: Average production by country?\n",
    "print(\"\\nWith TIDY data - Average by country:\")\n",
    "print(tidy_data.groupby('country')['wheat_production'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted to tidy format:\n",
      "  country         indicator  year  production\n",
      "0  Canada  wheat_production  2020       35183\n",
      "1     USA  wheat_production  2020       49691\n",
      "2  Mexico  wheat_production  2020        3115\n",
      "3  Canada  wheat_production  2021       22296\n",
      "4     USA  wheat_production  2021       44790\n",
      "5  Mexico  wheat_production  2021        3024\n",
      "6  Canada  wheat_production  2022       34335\n",
      "7     USA  wheat_production  2022       44900\n",
      "8  Mexico  wheat_production  2022        3195\n"
     ]
    }
   ],
   "source": [
    "# Converting messy to tidy using pd.melt()\n",
    "\n",
    "tidy_from_messy = pd.melt(\n",
    "    messy_data,\n",
    "    id_vars=['country', 'indicator'],    # Columns to keep\n",
    "    value_vars=['2020', '2021', '2022'], # Columns to unpivot\n",
    "    var_name='year',                      # Name for the new column\n",
    "    value_name='production'               # Name for the values\n",
    ")\n",
    "\n",
    "# Convert year to integer\n",
    "tidy_from_messy['year'] = tidy_from_messy['year'].astype(int)\n",
    "\n",
    "print(\"Converted to tidy format:\")\n",
    "print(tidy_from_messy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Messy Data Patterns\n",
    "\n",
    "| Pattern | Problem | Solution |\n",
    "|---------|---------|----------|\n",
    "| Column headers are values | Years as columns | `pd.melt()` |\n",
    "| Multiple variables in one column | \"temp_min_max\" | `str.split()` + `pd.concat()` |\n",
    "| Variables in rows and columns | Crosstab format | `pd.melt()` + `pd.pivot()` |\n",
    "| Multiple types in one column | \"100 kg\" mixed with \"50 lbs\" | Extract + convert |\n",
    "| One observation across multiple rows | Header row + data row | Combine rows |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Column Typing and Type Coercion\n",
    "\n",
    "### Why Types Matter\n",
    "\n",
    "Data types determine:\n",
    "- What operations are valid (can't average strings)\n",
    "- Memory usage (int8 vs int64)\n",
    "- Behavior (\"1\" + \"2\" = \"12\" vs 1 + 2 = 3)\n",
    "\n",
    "### Pandas Data Types\n",
    "\n",
    "| Type | Description | Example |\n",
    "|------|-------------|----------|\n",
    "| `int64` | Integer | 42, -7, 1000 |\n",
    "| `float64` | Decimal | 3.14, -0.5 |\n",
    "| `object` | String/mixed | \"Canada\", \"N/A\" |\n",
    "| `bool` | Boolean | True, False |\n",
    "| `datetime64` | Date/time | 2024-01-15 |\n",
    "| `category` | Categorical | Low/Medium/High |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messy FAO Food Price Index Data:\n",
      "       date cereals_index meat_index dairy_index oils_index  sugar_index\n",
      "0   2023-01         147.2      113.8       126.5     156.3*        115.0\n",
      "1   2023-02         149.5      114,2       130.2      154.2        116.2\n",
      "2   2023-03         148.3      114.1       128.7    149.8**        118.5\n",
      "3   2023-04         146.1      114.0       125.3      142.5        119.8\n",
      "4   2023-05         139.2      113.9       120.1      127.8        122.3\n",
      "5   2023-06         129.8      113.4       115.8      118.5        125.6\n",
      "6   2023-07         127.5         ..       111.2      119.8        128.9\n",
      "7   2023-08         130.2      115.2       108.5      129.3        131.2\n",
      "8   2023-09         132.1      116.8       110.3      130.7        129.5\n",
      "9   2023-10         131.8        N/A       115.7      126.5        127.8\n",
      "10  2023-11         127.4      118.3       118.2      128.7        130.1\n",
      "11  2023-12         125.5      118.9       116.8      126.3        131.5\n",
      "\n",
      "Data types:\n",
      "date              object\n",
      "cereals_index     object\n",
      "meat_index        object\n",
      "dairy_index       object\n",
      "oils_index        object\n",
      "sugar_index      float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Let's create a realistic messy dataset: FAO Food Price Index\n",
    "# This simulates common issues you'll encounter\n",
    "\n",
    "food_prices_messy = pd.DataFrame({\n",
    "    'date': ['2023-01', '2023-02', '2023-03', '2023-04', '2023-05', '2023-06',\n",
    "             '2023-07', '2023-08', '2023-09', '2023-10', '2023-11', '2023-12'],\n",
    "    'cereals_index': ['147.2', '149.5', '148.3', '146.1', '139.2', '129.8',\n",
    "                      '127.5', '130.2', '132.1', '131.8', '127.4', '125.5'],\n",
    "    'meat_index': ['113.8', '114,2', '114.1', '114.0', '113.9', '113.4',  # Note: European decimal\n",
    "                   '..', '115.2', '116.8', 'N/A', '118.3', '118.9'],       # Note: Missing values\n",
    "    'dairy_index': ['126.5', '130.2', '128.7', '125.3', '120.1', '115.8',\n",
    "                    '111.2', '108.5', '110.3', '115.7', '118.2', '116.8'],\n",
    "    'oils_index': ['156.3*', '154.2', '149.8**', '142.5', '127.8', '118.5',  # Note: Footnotes\n",
    "                   '119.8', '129.3', '130.7', '126.5', '128.7', '126.3'],\n",
    "    'sugar_index': [115, 116.2, 118.5, 119.8, 122.3, 125.6,  # Note: Mixed int/float\n",
    "                    128.9, 131.2, 129.5, 127.8, 130.1, 131.5]\n",
    "})\n",
    "\n",
    "print(\"Messy FAO Food Price Index Data:\")\n",
    "print(food_prices_messy)\n",
    "print(f\"\\nData types:\")\n",
    "print(food_prices_messy.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Analysis\n",
    "\n",
    "Looking at the data above:\n",
    "1. **date** - String, needs to be datetime\n",
    "2. **cereals_index** - String numbers, needs float\n",
    "3. **meat_index** - Has European decimal (114,2), missing codes (.., N/A)\n",
    "4. **dairy_index** - String numbers\n",
    "5. **oils_index** - Has footnote markers (*, **)\n",
    "6. **sugar_index** - Already numeric (mixed int/float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 - Date converted:\n",
      "  Type: datetime64[ns]\n",
      "  Sample: 2023-01-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Step-by-step type cleaning\n",
    "\n",
    "df = food_prices_messy.copy()\n",
    "\n",
    "# 1. Convert date to datetime\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y-%m')\n",
    "print(\"Step 1 - Date converted:\")\n",
    "print(f\"  Type: {df['date'].dtype}\")\n",
    "print(f\"  Sample: {df['date'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2 - Numeric columns cleaned:\n",
      "date             datetime64[ns]\n",
      "cereals_index           float64\n",
      "meat_index              float64\n",
      "dairy_index             float64\n",
      "oils_index              float64\n",
      "sugar_index             float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 2. Clean numeric columns - Create a reusable function\n",
    "\n",
    "def clean_numeric_column(series):\n",
    "    \"\"\"\n",
    "    Clean a column that should be numeric but has:\n",
    "    - European decimals (comma instead of period)\n",
    "    - Missing value codes (.., N/A, NA, etc.)\n",
    "    - Footnote markers (*, **, E, F)\n",
    "    - Whitespace\n",
    "    \n",
    "    Returns a float series with proper NaN for missing.\n",
    "    \"\"\"\n",
    "    # Convert to string first\n",
    "    s = series.astype(str)\n",
    "    \n",
    "    # Strip whitespace\n",
    "    s = s.str.strip()\n",
    "    \n",
    "    # Replace common missing value codes with empty string\n",
    "    missing_codes = ['..', 'N/A', 'NA', 'n/a', 'na', '-', '', 'None', 'null', 'NULL']\n",
    "    for code in missing_codes:\n",
    "        s = s.replace(code, '')\n",
    "    \n",
    "    # Remove footnote markers (*, **, E, F at end)\n",
    "    s = s.str.replace(r'[*EFef]+$', '', regex=True)\n",
    "    \n",
    "    # Handle European decimals: if comma exists and no period, replace comma with period\n",
    "    def fix_european_decimal(val):\n",
    "        if ',' in val and '.' not in val:\n",
    "            return val.replace(',', '.')\n",
    "        return val\n",
    "    \n",
    "    s = s.apply(fix_european_decimal)\n",
    "    \n",
    "    # Convert to numeric (empty strings become NaN)\n",
    "    return pd.to_numeric(s, errors='coerce')\n",
    "\n",
    "\n",
    "# Apply to all index columns\n",
    "index_columns = ['cereals_index', 'meat_index', 'dairy_index', 'oils_index']\n",
    "\n",
    "for col in index_columns:\n",
    "    df[col] = clean_numeric_column(df[col])\n",
    "\n",
    "print(\"Step 2 - Numeric columns cleaned:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned FAO Food Price Index:\n",
      "         date  cereals_index  meat_index  dairy_index  oils_index  sugar_index\n",
      "0  2023-01-01          147.2       113.8        126.5       156.3        115.0\n",
      "1  2023-02-01          149.5       114.2        130.2       154.2        116.2\n",
      "2  2023-03-01          148.3       114.1        128.7       149.8        118.5\n",
      "3  2023-04-01          146.1       114.0        125.3       142.5        119.8\n",
      "4  2023-05-01          139.2       113.9        120.1       127.8        122.3\n",
      "5  2023-06-01          129.8       113.4        115.8       118.5        125.6\n",
      "6  2023-07-01          127.5         NaN        111.2       119.8        128.9\n",
      "7  2023-08-01          130.2       115.2        108.5       129.3        131.2\n",
      "8  2023-09-01          132.1       116.8        110.3       130.7        129.5\n",
      "9  2023-10-01          131.8         NaN        115.7       126.5        127.8\n",
      "10 2023-11-01          127.4       118.3        118.2       128.7        130.1\n",
      "11 2023-12-01          125.5       118.9        116.8       126.3        131.5\n",
      "\n",
      "Missing values per column:\n",
      "date             0\n",
      "cereals_index    0\n",
      "meat_index       2\n",
      "dairy_index      0\n",
      "oils_index       0\n",
      "sugar_index      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# View the cleaned data\n",
    "print(\"Cleaned FAO Food Price Index:\")\n",
    "print(df)\n",
    "print(f\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type Coercion Methods Summary\n",
    "\n",
    "| Method | Use Case | Example |\n",
    "|--------|----------|----------|\n",
    "| `astype(int)` | Clean integers | `df['year'].astype(int)` |\n",
    "| `astype(float)` | Clean floats | `df['price'].astype(float)` |\n",
    "| `pd.to_numeric()` | With errors handling | `pd.to_numeric(s, errors='coerce')` |\n",
    "| `pd.to_datetime()` | Date parsing | `pd.to_datetime(s, format='%Y-%m-%d')` |\n",
    "| `astype('category')` | Categorical data | `df['region'].astype('category')` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical column:\n",
      "        date  cereals_index assessment\n",
      "0 2023-01-01          147.2       High\n",
      "1 2023-02-01          149.5       High\n",
      "2 2023-03-01          148.3       High\n",
      "3 2023-04-01          146.1       High\n",
      "4 2023-05-01          139.2     Medium\n",
      "\n",
      "Type: category\n",
      "Categories: ['Low', 'Medium', 'High']\n"
     ]
    }
   ],
   "source": [
    "# Special case: Categorical data\n",
    "# Good for columns with limited distinct values\n",
    "\n",
    "# Add a region column\n",
    "df['assessment'] = pd.cut(\n",
    "    df['cereals_index'],\n",
    "    bins=[0, 130, 145, 200],\n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")\n",
    "\n",
    "print(\"Categorical column:\")\n",
    "print(df[['date', 'cereals_index', 'assessment']].head())\n",
    "print(f\"\\nType: {df['assessment'].dtype}\")\n",
    "print(f\"Categories: {df['assessment'].cat.categories.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Reshaping Data: Wide to Long\n",
    "\n",
    "### When to Reshape\n",
    "\n",
    "- **Wide → Long (melt)**: When column headers contain data values (years, categories)\n",
    "- **Long → Wide (pivot)**: When you need columns for comparison/visualization\n",
    "\n",
    "### Real Example: Global Temperature Anomalies\n",
    "\n",
    "Temperature data often comes with months as columns - classic wide format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WIDE FORMAT (months as columns):\n",
      "  country  year   Jan   Feb   Mar   Apr   May   Jun\n",
      "0  Canada  2021  0.85  0.92  1.05  0.78  0.65  0.72\n",
      "1  Canada  2022  1.12  0.88  1.18  0.95  0.82  0.88\n",
      "2  Canada  2023  0.95  1.05  1.32  1.15  0.98  1.02\n",
      "3     USA  2021  1.02  1.15  1.28  0.92  0.78  0.85\n",
      "4     USA  2022  1.35  1.22  1.45  1.08  0.95  1.02\n",
      "5     USA  2023  1.18  1.08  1.35  1.22  1.05  1.15\n",
      "6  Brazil  2021  0.62  0.58  0.72  0.55  0.42  0.38\n",
      "7  Brazil  2022  0.78  0.82  0.95  0.68  0.55  0.48\n",
      "8  Brazil  2023  0.95  0.88  1.02  0.85  0.72  0.62\n",
      "\n",
      "Shape: (9, 8)\n"
     ]
    }
   ],
   "source": [
    "# Create temperature anomaly data (similar to NOAA format)\n",
    "\n",
    "temp_wide = pd.DataFrame({\n",
    "    'country': ['Canada', 'Canada', 'Canada', 'USA', 'USA', 'USA', \n",
    "                'Brazil', 'Brazil', 'Brazil'],\n",
    "    'year': [2021, 2022, 2023, 2021, 2022, 2023, 2021, 2022, 2023],\n",
    "    'Jan': [0.85, 1.12, 0.95, 1.02, 1.35, 1.18, 0.62, 0.78, 0.95],\n",
    "    'Feb': [0.92, 0.88, 1.05, 1.15, 1.22, 1.08, 0.58, 0.82, 0.88],\n",
    "    'Mar': [1.05, 1.18, 1.32, 1.28, 1.45, 1.35, 0.72, 0.95, 1.02],\n",
    "    'Apr': [0.78, 0.95, 1.15, 0.92, 1.08, 1.22, 0.55, 0.68, 0.85],\n",
    "    'May': [0.65, 0.82, 0.98, 0.78, 0.95, 1.05, 0.42, 0.55, 0.72],\n",
    "    'Jun': [0.72, 0.88, 1.02, 0.85, 1.02, 1.15, 0.38, 0.48, 0.62]\n",
    "})\n",
    "\n",
    "print(\"WIDE FORMAT (months as columns):\")\n",
    "print(temp_wide)\n",
    "print(f\"\\nShape: {temp_wide.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LONG FORMAT (tidy):\n",
      "   country  year month  temp_anomaly\n",
      "0   Canada  2021   Jan          0.85\n",
      "1   Canada  2022   Jan          1.12\n",
      "2   Canada  2023   Jan          0.95\n",
      "3      USA  2021   Jan          1.02\n",
      "4      USA  2022   Jan          1.35\n",
      "5      USA  2023   Jan          1.18\n",
      "6   Brazil  2021   Jan          0.62\n",
      "7   Brazil  2022   Jan          0.78\n",
      "8   Brazil  2023   Jan          0.95\n",
      "9   Canada  2021   Feb          0.92\n",
      "10  Canada  2022   Feb          0.88\n",
      "11  Canada  2023   Feb          1.05\n",
      "\n",
      "Shape: (54, 4)\n"
     ]
    }
   ],
   "source": [
    "# Convert to long format using pd.melt()\n",
    "\n",
    "temp_long = pd.melt(\n",
    "    temp_wide,\n",
    "    id_vars=['country', 'year'],           # Keep these as identifiers\n",
    "    value_vars=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun'],  # Columns to unpivot\n",
    "    var_name='month',                       # New column name for former headers\n",
    "    value_name='temp_anomaly'               # New column name for values\n",
    ")\n",
    "\n",
    "print(\"LONG FORMAT (tidy):\")\n",
    "print(temp_long.head(12))\n",
    "print(f\"\\nShape: {temp_long.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average temperature anomaly by country:\n",
      "country\n",
      "Brazil    0.70\n",
      "Canada    0.96\n",
      "USA       1.12\n",
      "Name: temp_anomaly, dtype: float64\n",
      "\n",
      "Average anomaly by year:\n",
      "year\n",
      "2021    0.79\n",
      "2022    0.95\n",
      "2023    1.03\n",
      "Name: temp_anomaly, dtype: float64\n",
      "\n",
      "Hottest months (highest anomaly):\n",
      "   country  year month  temp_anomaly\n",
      "22     USA  2022   Mar          1.45\n",
      "4      USA  2022   Jan          1.35\n",
      "23     USA  2023   Mar          1.35\n",
      "20  Canada  2023   Mar          1.32\n",
      "21     USA  2021   Mar          1.28\n"
     ]
    }
   ],
   "source": [
    "# Now analysis is easy!\n",
    "\n",
    "# Average anomaly by country\n",
    "print(\"Average temperature anomaly by country:\")\n",
    "print(temp_long.groupby('country')['temp_anomaly'].mean().round(2))\n",
    "\n",
    "print(\"\\nAverage anomaly by year:\")\n",
    "print(temp_long.groupby('year')['temp_anomaly'].mean().round(2))\n",
    "\n",
    "print(\"\\nHottest months (highest anomaly):\")\n",
    "print(temp_long.nlargest(5, 'temp_anomaly')[['country', 'year', 'month', 'temp_anomaly']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Back to WIDE FORMAT:\n",
      "month country  year   Apr   Feb   Jan   Jun   Mar   May\n",
      "0      Brazil  2021  0.55  0.58  0.62  0.38  0.72  0.42\n",
      "1      Brazil  2022  0.68  0.82  0.78  0.48  0.95  0.55\n",
      "2      Brazil  2023  0.85  0.88  0.95  0.62  1.02  0.72\n",
      "3      Canada  2021  0.78  0.92  0.85  0.72  1.05  0.65\n",
      "4      Canada  2022  0.95  0.88  1.12  0.88  1.18  0.82\n",
      "5      Canada  2023  1.15  1.05  0.95  1.02  1.32  0.98\n",
      "6         USA  2021  0.92  1.15  1.02  0.85  1.28  0.78\n",
      "7         USA  2022  1.08  1.22  1.35  1.02  1.45  0.95\n",
      "8         USA  2023  1.22  1.08  1.18  1.15  1.35  1.05\n"
     ]
    }
   ],
   "source": [
    "# Going back: Long to Wide using pivot()\n",
    "\n",
    "temp_back_wide = temp_long.pivot(\n",
    "    index=['country', 'year'],  # Rows\n",
    "    columns='month',             # Columns\n",
    "    values='temp_anomaly'        # Values\n",
    ").reset_index()\n",
    "\n",
    "print(\"Back to WIDE FORMAT:\")\n",
    "print(temp_back_wide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape Cheat Sheet\n",
    "\n",
    "```python\n",
    "# Wide to Long (unpivot)\n",
    "pd.melt(df, id_vars=['key_cols'], value_vars=['cols_to_unpivot'],\n",
    "        var_name='new_col_name', value_name='value_col_name')\n",
    "\n",
    "# Long to Wide (pivot)\n",
    "df.pivot(index='row_key', columns='col_to_spread', values='values')\n",
    "\n",
    "# Long to Wide with aggregation\n",
    "df.pivot_table(index='row_key', columns='col_to_spread', \n",
    "               values='values', aggfunc='mean')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Missing Data Strategies\n",
    "\n",
    "### Types of Missing Data\n",
    "\n",
    "Understanding **why** data is missing helps decide how to handle it:\n",
    "\n",
    "| Type | Description | Example | Strategy |\n",
    "|------|-------------|---------|----------|\n",
    "| **MCAR** | Missing Completely At Random | Sensor malfunction | Drop or impute |\n",
    "| **MAR** | Missing At Random (depends on other variables) | Rich countries report more | Impute with care |\n",
    "| **MNAR** | Missing Not At Random (depends on missing value itself) | High pollution not reported | Cannot ignore, needs modeling |\n",
    "\n",
    "### Real Example: Water Quality Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Water Quality Dataset:\n",
      "  station_id region sample_date    ph  dissolved_oxygen  turbidity  \\\n",
      "0     WQ_001   East  2023-01-01   NaN             13.56       1.39   \n",
      "1     WQ_002   West  2023-01-08   NaN              7.12       5.58   \n",
      "2     WQ_003  North  2023-01-15  8.48              7.90       7.17   \n",
      "3     WQ_004   East  2023-01-22   NaN              8.41       1.36   \n",
      "4     WQ_005   East  2023-01-29  7.26              6.37       6.51   \n",
      "5     WQ_006   West  2023-02-05  6.94             10.06       2.29   \n",
      "6     WQ_007  North  2023-02-12   NaN              9.86       5.00   \n",
      "7     WQ_008  North  2023-02-19  7.67              8.53       5.02   \n",
      "8     WQ_009   East  2023-02-26  7.35              7.70       3.84   \n",
      "9     WQ_010  South  2023-03-05  6.88              6.26       0.47   \n",
      "\n",
      "   temperature_c  nitrate_mg_l  phosphate_mg_l  \n",
      "0           15.9          4.98           0.627  \n",
      "1            8.9           NaN           0.540  \n",
      "2           20.2          7.33           0.164  \n",
      "3           21.6          2.15           1.549  \n",
      "4           18.7          2.09           0.670  \n",
      "5           10.2           NaN           0.404  \n",
      "6           11.2          3.15           0.473  \n",
      "7            9.3          3.63             NaN  \n",
      "8           18.8          4.77           0.142  \n",
      "9           21.3          6.62           0.220  \n"
     ]
    }
   ],
   "source": [
    "# Create water quality dataset with realistic missing patterns\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n_samples = 50\n",
    "water_quality = pd.DataFrame({\n",
    "    'station_id': [f'WQ_{i:03d}' for i in range(1, n_samples + 1)],\n",
    "    'region': np.random.choice(['North', 'South', 'East', 'West'], n_samples),\n",
    "    'sample_date': pd.date_range('2023-01-01', periods=n_samples, freq='W'),\n",
    "    'ph': np.random.normal(7.2, 0.5, n_samples).round(2),\n",
    "    'dissolved_oxygen': np.random.normal(8.5, 1.5, n_samples).round(2),\n",
    "    'turbidity': np.random.exponential(5, n_samples).round(2),\n",
    "    'temperature_c': np.random.normal(15, 5, n_samples).round(1),\n",
    "    'nitrate_mg_l': np.random.exponential(3, n_samples).round(2),\n",
    "    'phosphate_mg_l': np.random.exponential(0.5, n_samples).round(3)\n",
    "})\n",
    "\n",
    "# Introduce realistic missing patterns\n",
    "# MCAR: Random sensor failures (pH)\n",
    "mcar_mask = np.random.random(n_samples) < 0.1\n",
    "water_quality.loc[mcar_mask, 'ph'] = np.nan\n",
    "\n",
    "# MAR: Northern stations have more missing dissolved oxygen (harsh conditions)\n",
    "mar_mask = (water_quality['region'] == 'North') & (np.random.random(n_samples) < 0.3)\n",
    "water_quality.loc[mar_mask, 'dissolved_oxygen'] = np.nan\n",
    "\n",
    "# MNAR: High turbidity readings often fail (too murky for sensor)\n",
    "mnar_mask = water_quality['turbidity'] > 10\n",
    "water_quality.loc[mnar_mask, 'turbidity'] = np.nan\n",
    "\n",
    "# Additional random missing\n",
    "for col in ['nitrate_mg_l', 'phosphate_mg_l']:\n",
    "    mask = np.random.random(n_samples) < 0.15\n",
    "    water_quality.loc[mask, col] = np.nan\n",
    "\n",
    "print(\"Water Quality Dataset:\")\n",
    "print(water_quality.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Value Summary:\n",
      "==================================================\n",
      "                  missing_count  missing_pct    dtype\n",
      "ph                            6         12.0  float64\n",
      "dissolved_oxygen              4          8.0  float64\n",
      "turbidity                     8         16.0  float64\n",
      "nitrate_mg_l                  6         12.0  float64\n",
      "phosphate_mg_l                7         14.0  float64\n"
     ]
    }
   ],
   "source": [
    "# Analyze missing data patterns\n",
    "\n",
    "print(\"Missing Value Summary:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "missing_summary = pd.DataFrame({\n",
    "    'missing_count': water_quality.isnull().sum(),\n",
    "    'missing_pct': (water_quality.isnull().sum() / len(water_quality) * 100).round(1),\n",
    "    'dtype': water_quality.dtypes\n",
    "})\n",
    "\n",
    "print(missing_summary[missing_summary['missing_count'] > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing dissolved_oxygen by region:\n",
      "region\n",
      "East      0/13 (0%)\n",
      "North    4/10 (40%)\n",
      "South     0/11 (0%)\n",
      "West      0/16 (0%)\n",
      "Name: dissolved_oxygen, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check for patterns in missing data\n",
    "\n",
    "print(\"\\nMissing dissolved_oxygen by region:\")\n",
    "print(water_quality.groupby('region')['dissolved_oxygen'].apply(\n",
    "    lambda x: f\"{x.isnull().sum()}/{len(x)} ({x.isnull().mean()*100:.0f}%)\"\n",
    "))\n",
    "\n",
    "# This confirms MAR pattern: North has more missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Data: Options\n",
    "\n",
    "| Strategy | Method | When to Use |\n",
    "|----------|--------|-------------|\n",
    "| **Drop rows** | `dropna()` | Few missing, MCAR |\n",
    "| **Drop columns** | `drop()` | >50% missing |\n",
    "| **Fill with constant** | `fillna(value)` | Meaningful default |\n",
    "| **Fill with mean/median** | `fillna(df.mean())` | MCAR, continuous |\n",
    "| **Fill with mode** | `fillna(df.mode()[0])` | Categorical |\n",
    "| **Forward/backward fill** | `ffill()`, `bfill()` | Time series |\n",
    "| **Interpolation** | `interpolate()` | Ordered data |\n",
    "| **Group-based imputation** | `groupby().transform()` | MAR |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original rows: 50\n",
      "After dropna(): 28\n",
      "Lost: 22 rows (44%)\n"
     ]
    }
   ],
   "source": [
    "# Strategy 1: Drop rows with any missing (aggressive)\n",
    "\n",
    "df_dropped = water_quality.dropna()\n",
    "print(f\"Original rows: {len(water_quality)}\")\n",
    "print(f\"After dropna(): {len(df_dropped)}\")\n",
    "print(f\"Lost: {len(water_quality) - len(df_dropped)} rows ({(1 - len(df_dropped)/len(water_quality))*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pH filled with mean:\n",
      "  Original missing: 6\n",
      "  After filling: 0\n",
      "  Fill value used: 7.28\n"
     ]
    }
   ],
   "source": [
    "# Strategy 2: Fill with column mean (simple but loses variance)\n",
    "\n",
    "df_mean_filled = water_quality.copy()\n",
    "df_mean_filled['ph'] = df_mean_filled['ph'].fillna(df_mean_filled['ph'].mean())\n",
    "\n",
    "print(\"pH filled with mean:\")\n",
    "print(f\"  Original missing: {water_quality['ph'].isnull().sum()}\")\n",
    "print(f\"  After filling: {df_mean_filled['ph'].isnull().sum()}\")\n",
    "print(f\"  Fill value used: {water_quality['ph'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dissolved oxygen filled with regional mean:\n",
      "  Original missing: 4\n",
      "  After filling: 0\n",
      "\n",
      "Regional means used:\n",
      "region\n",
      "East     8.40\n",
      "North    8.87\n",
      "South    7.42\n",
      "West     8.75\n",
      "Name: dissolved_oxygen, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Strategy 3: Group-based imputation (better for MAR)\n",
    "# Fill dissolved_oxygen with regional mean\n",
    "\n",
    "df_group_filled = water_quality.copy()\n",
    "\n",
    "# Calculate regional means\n",
    "regional_means = water_quality.groupby('region')['dissolved_oxygen'].transform('mean')\n",
    "\n",
    "# Fill missing with regional mean\n",
    "df_group_filled['dissolved_oxygen'] = df_group_filled['dissolved_oxygen'].fillna(regional_means)\n",
    "\n",
    "print(\"Dissolved oxygen filled with regional mean:\")\n",
    "print(f\"  Original missing: {water_quality['dissolved_oxygen'].isnull().sum()}\")\n",
    "print(f\"  After filling: {df_group_filled['dissolved_oxygen'].isnull().sum()}\")\n",
    "\n",
    "print(\"\\nRegional means used:\")\n",
    "print(water_quality.groupby('region')['dissolved_oxygen'].mean().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data with missing flags:\n",
      "  station_id  turbidity  turbidity_missing\n",
      "0     WQ_001       1.39                  0\n",
      "1     WQ_002       5.58                  0\n",
      "2     WQ_003       7.17                  0\n",
      "3     WQ_004       1.36                  0\n",
      "4     WQ_005       6.51                  0\n",
      "5     WQ_006       2.29                  0\n",
      "6     WQ_007       5.00                  0\n",
      "7     WQ_008       5.02                  0\n",
      "8     WQ_009       3.84                  0\n",
      "9     WQ_010       0.47                  0\n"
     ]
    }
   ],
   "source": [
    "# Strategy 4: Flag missing values (preserve information)\n",
    "# Sometimes it's important to know data WAS missing\n",
    "\n",
    "df_flagged = water_quality.copy()\n",
    "\n",
    "# Create flag columns\n",
    "for col in ['turbidity', 'nitrate_mg_l', 'phosphate_mg_l']:\n",
    "    df_flagged[f'{col}_missing'] = df_flagged[col].isnull().astype(int)\n",
    "\n",
    "# Fill with median\n",
    "for col in ['turbidity', 'nitrate_mg_l', 'phosphate_mg_l']:\n",
    "    df_flagged[col] = df_flagged[col].fillna(df_flagged[col].median())\n",
    "\n",
    "print(\"Data with missing flags:\")\n",
    "print(df_flagged[['station_id', 'turbidity', 'turbidity_missing']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices for Missing Data\n",
    "\n",
    "1. **Always document** what was missing and how you handled it\n",
    "2. **Understand the mechanism** - MCAR vs MAR vs MNAR\n",
    "3. **Don't impute too aggressively** - >30% missing might mean drop the column\n",
    "4. **Consider flagging** - Missing-ness itself can be informative\n",
    "5. **Check impact** - Compare results with different strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Validation Checks: Ranges, Nulls, and Keys\n",
    "\n",
    "### Why Validate?\n",
    "\n",
    "Validation catches problems early:\n",
    "- Data entry errors (pH of 72 instead of 7.2)\n",
    "- Unit mismatches (Celsius vs Fahrenheit)\n",
    "- Duplicate records\n",
    "- Broken relationships between tables\n",
    "\n",
    "### Types of Validation\n",
    "\n",
    "| Type | Check | Example |\n",
    "|------|-------|----------|\n",
    "| **Range** | Values within bounds | pH between 0-14 |\n",
    "| **Null** | Required fields present | Station ID not null |\n",
    "| **Type** | Correct data type | Year is integer |\n",
    "| **Uniqueness** | No duplicates | Unique station-date combo |\n",
    "| **Referential** | Foreign keys valid | Country code exists |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation function defined.\n"
     ]
    }
   ],
   "source": [
    "# Create a validation framework\n",
    "\n",
    "def validate_dataframe(df, rules):\n",
    "    \"\"\"\n",
    "    Validate a DataFrame against a set of rules.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Data to validate\n",
    "    rules : list of dict\n",
    "        Each rule has: column, check_type, params, description\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Validation results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for rule in rules:\n",
    "        col = rule['column']\n",
    "        check = rule['check_type']\n",
    "        params = rule.get('params', {})\n",
    "        desc = rule['description']\n",
    "        \n",
    "        if col not in df.columns:\n",
    "            results.append({\n",
    "                'check': desc,\n",
    "                'passed': False,\n",
    "                'message': f\"Column '{col}' not found\"\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Range check\n",
    "        if check == 'range':\n",
    "            min_val = params.get('min')\n",
    "            max_val = params.get('max')\n",
    "            violations = df[\n",
    "                (df[col] < min_val) | (df[col] > max_val)\n",
    "            ][col].dropna()\n",
    "            passed = len(violations) == 0\n",
    "            msg = f\"{len(violations)} values outside [{min_val}, {max_val}]\" if not passed else \"OK\"\n",
    "        \n",
    "        # Not null check\n",
    "        elif check == 'not_null':\n",
    "            null_count = df[col].isnull().sum()\n",
    "            passed = null_count == 0\n",
    "            msg = f\"{null_count} null values\" if not passed else \"OK\"\n",
    "        \n",
    "        # Unique check\n",
    "        elif check == 'unique':\n",
    "            dup_count = df[col].duplicated().sum()\n",
    "            passed = dup_count == 0\n",
    "            msg = f\"{dup_count} duplicate values\" if not passed else \"OK\"\n",
    "        \n",
    "        # Type check\n",
    "        elif check == 'dtype':\n",
    "            expected = params.get('expected')\n",
    "            actual = str(df[col].dtype)\n",
    "            passed = expected in actual\n",
    "            msg = f\"Expected {expected}, got {actual}\" if not passed else \"OK\"\n",
    "        \n",
    "        # Values in set\n",
    "        elif check == 'in_set':\n",
    "            valid_values = params.get('values', [])\n",
    "            invalid = df[~df[col].isin(valid_values)][col].dropna().unique()\n",
    "            passed = len(invalid) == 0\n",
    "            msg = f\"Invalid values: {list(invalid)[:5]}\" if not passed else \"OK\"\n",
    "        \n",
    "        else:\n",
    "            passed = False\n",
    "            msg = f\"Unknown check type: {check}\"\n",
    "        \n",
    "        results.append({\n",
    "            'check': desc,\n",
    "            'passed': passed,\n",
    "            'message': msg\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Validation function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION RESULTS\n",
      "============================================================\n",
      "[OK] Station ID must not be null\n",
      "[OK] Sample date must not be null\n",
      "[OK] Station ID must be unique\n",
      "[OK] pH must be between 0 and 14\n",
      "[OK] Dissolved oxygen must be 0-20 mg/L\n",
      "[OK] Temperature must be -5 to 40 C\n",
      "[OK] Turbidity must be 0-1000 NTU\n",
      "[OK] Region must be valid\n",
      "[OK] Sample date must be datetime\n",
      "\n",
      "============================================================\n",
      "Result: 9/9 checks passed\n"
     ]
    }
   ],
   "source": [
    "# Define validation rules for water quality data\n",
    "\n",
    "water_quality_rules = [\n",
    "    # Not null checks\n",
    "    {'column': 'station_id', 'check_type': 'not_null', \n",
    "     'description': 'Station ID must not be null'},\n",
    "    {'column': 'sample_date', 'check_type': 'not_null', \n",
    "     'description': 'Sample date must not be null'},\n",
    "    \n",
    "    # Uniqueness\n",
    "    {'column': 'station_id', 'check_type': 'unique', \n",
    "     'description': 'Station ID must be unique'},\n",
    "    \n",
    "    # Range checks\n",
    "    {'column': 'ph', 'check_type': 'range', \n",
    "     'params': {'min': 0, 'max': 14},\n",
    "     'description': 'pH must be between 0 and 14'},\n",
    "    {'column': 'dissolved_oxygen', 'check_type': 'range', \n",
    "     'params': {'min': 0, 'max': 20},\n",
    "     'description': 'Dissolved oxygen must be 0-20 mg/L'},\n",
    "    {'column': 'temperature_c', 'check_type': 'range', \n",
    "     'params': {'min': -5, 'max': 40},\n",
    "     'description': 'Temperature must be -5 to 40 C'},\n",
    "    {'column': 'turbidity', 'check_type': 'range', \n",
    "     'params': {'min': 0, 'max': 1000},\n",
    "     'description': 'Turbidity must be 0-1000 NTU'},\n",
    "    \n",
    "    # Category check\n",
    "    {'column': 'region', 'check_type': 'in_set', \n",
    "     'params': {'values': ['North', 'South', 'East', 'West']},\n",
    "     'description': 'Region must be valid'},\n",
    "    \n",
    "    # Type check\n",
    "    {'column': 'sample_date', 'check_type': 'dtype', \n",
    "     'params': {'expected': 'datetime'},\n",
    "     'description': 'Sample date must be datetime'},\n",
    "]\n",
    "\n",
    "# Run validation\n",
    "validation_results = validate_dataframe(water_quality, water_quality_rules)\n",
    "\n",
    "# Display results\n",
    "print(\"VALIDATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "for result in validation_results:\n",
    "    status = \"PASS\" if result['passed'] else \"FAIL\"\n",
    "    symbol = \"[OK]\" if result['passed'] else \"[X] \"\n",
    "    print(f\"{symbol} {result['check']}\")\n",
    "    if not result['passed']:\n",
    "        print(f\"      {result['message']}\")\n",
    "\n",
    "# Summary\n",
    "passed = sum(1 for r in validation_results if r['passed'])\n",
    "total = len(validation_results)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Result: {passed}/{total} checks passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PASS: ['station_id'] forms a unique key\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Composite key uniqueness check\n",
    "# Often need to check uniqueness of multiple columns together\n",
    "\n",
    "def check_composite_key(df, key_columns):\n",
    "    \"\"\"\n",
    "    Check if combination of columns forms a unique key.\n",
    "    \"\"\"\n",
    "    duplicates = df.duplicated(subset=key_columns, keep=False)\n",
    "    dup_count = duplicates.sum()\n",
    "    \n",
    "    if dup_count > 0:\n",
    "        print(f\"FAIL: {dup_count} rows have duplicate keys\")\n",
    "        print(\"\\nDuplicate examples:\")\n",
    "        print(df[duplicates][key_columns].head(10))\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"PASS: {key_columns} forms a unique key\")\n",
    "        return True\n",
    "\n",
    "\n",
    "# Test: station_id should be unique (single column key)\n",
    "check_composite_key(water_quality, ['station_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "VALIDATION REPORT: Water Quality Monitoring\n",
      "============================================================\n",
      "Generated: 2026-01-06 23:02:53\n",
      "\n",
      "--- BASIC INFO ---\n",
      "Rows: 50\n",
      "Columns: 9\n",
      "Memory: 8.9 KB\n",
      "\n",
      "--- MISSING VALUES ---\n",
      "                  count   pct\n",
      "ph                    6  12.0\n",
      "dissolved_oxygen      4   8.0\n",
      "turbidity             8  16.0\n",
      "nitrate_mg_l          6  12.0\n",
      "phosphate_mg_l        7  14.0\n",
      "\n",
      "--- DUPLICATES ---\n",
      "Duplicate rows: 0\n",
      "\n",
      "--- NUMERIC RANGES ---\n",
      "ph: [6.41, 8.59] (mean: 7.28)\n",
      "dissolved_oxygen: [4.52, 13.56] (mean: 8.35)\n",
      "turbidity: [0.08, 9.02] (mean: 3.41)\n",
      "temperature_c: [8.00, 28.70] (mean: 16.01)\n",
      "nitrate_mg_l: [0.08, 11.17] (mean: 2.57)\n",
      "phosphate_mg_l: [0.01, 1.62] (mean: 0.46)\n",
      "\n",
      "--- DATA TYPES ---\n",
      "station_id                  object\n",
      "region                      object\n",
      "sample_date         datetime64[ns]\n",
      "ph                         float64\n",
      "dissolved_oxygen           float64\n",
      "turbidity                  float64\n",
      "temperature_c              float64\n",
      "nitrate_mg_l               float64\n",
      "phosphate_mg_l             float64\n",
      "dtype: object\n",
      "\n",
      "============================================================\n",
      "END OF REPORT\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Creating a validation report\n",
    "\n",
    "def create_validation_report(df, name='Dataset'):\n",
    "    \"\"\"\n",
    "    Create a comprehensive validation report for a DataFrame.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"VALIDATION REPORT: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"\\n--- BASIC INFO ---\")\n",
    "    print(f\"Rows: {len(df):,}\")\n",
    "    print(f\"Columns: {len(df.columns)}\")\n",
    "    print(f\"Memory: {df.memory_usage(deep=True).sum() / 1024:.1f} KB\")\n",
    "    \n",
    "    # Missing values\n",
    "    print(f\"\\n--- MISSING VALUES ---\")\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df) * 100).round(1)\n",
    "    missing_df = pd.DataFrame({'count': missing, 'pct': missing_pct})\n",
    "    missing_df = missing_df[missing_df['count'] > 0]\n",
    "    if len(missing_df) > 0:\n",
    "        print(missing_df)\n",
    "    else:\n",
    "        print(\"No missing values!\")\n",
    "    \n",
    "    # Duplicates\n",
    "    print(f\"\\n--- DUPLICATES ---\")\n",
    "    dup_rows = df.duplicated().sum()\n",
    "    print(f\"Duplicate rows: {dup_rows}\")\n",
    "    \n",
    "    # Numeric column stats\n",
    "    print(f\"\\n--- NUMERIC RANGES ---\")\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        print(f\"{col}: [{df[col].min():.2f}, {df[col].max():.2f}] (mean: {df[col].mean():.2f})\")\n",
    "    \n",
    "    # Data types\n",
    "    print(f\"\\n--- DATA TYPES ---\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"END OF REPORT\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "\n",
    "# Generate report\n",
    "create_validation_report(water_quality, 'Water Quality Monitoring')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Key Takeaways\n",
    "\n",
    "### 1. Tidy Data\n",
    "- Each variable = column, each observation = row, each value = cell\n",
    "- Use `pd.melt()` to go from wide to long\n",
    "- Use `pd.pivot()` to go from long to wide\n",
    "\n",
    "### 2. Type Coercion\n",
    "- Always check `df.dtypes` after loading\n",
    "- Use `pd.to_numeric(errors='coerce')` for messy numbers\n",
    "- Use `pd.to_datetime()` with format strings\n",
    "- Create reusable cleaning functions\n",
    "\n",
    "### 3. Missing Data\n",
    "- Understand the mechanism (MCAR/MAR/MNAR)\n",
    "- Choose strategy based on mechanism and data\n",
    "- Document what you did\n",
    "- Consider flagging missing values\n",
    "\n",
    "### 4. Validation\n",
    "- Check ranges, nulls, types, uniqueness\n",
    "- Build validation functions for reuse\n",
    "- Create reports before analysis\n",
    "\n",
    "---\n",
    "\n",
    "## Next Class Preview\n",
    "\n",
    "**Lecture 6: Python Wrangling II**\n",
    "- Merging multiple data sources\n",
    "- Advanced pivot and unpivot\n",
    "- Building analysis-ready tables\n",
    "- Data contracts and documentation\n",
    "\n",
    "---\n",
    "\n",
    "## Practice Exercises\n",
    "\n",
    "1. **Exercise 1**: Create a messy dataset with years as columns and convert it to tidy format\n",
    "\n",
    "2. **Exercise 2**: Write a cleaning function for a column that has mixed formats (e.g., \"100 kg\", \"50.5kg\", \"75 KG\")\n",
    "\n",
    "3. **Exercise 3**: Analyze the missing data patterns in the water quality dataset and implement a group-based imputation for pH by region\n",
    "\n",
    "4. **Exercise 4**: Add three more validation rules to the water quality validation (hint: check for negative values, check date ranges)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ubc-mfre",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
